{"id":"ed670a48-ec04-45db-a405-ab35df4ee57f","zed":"context","version":"0.4.0","text":"\n\n\n```zig /home/msharran/.cache/zig/p/12205b2b47fe61a4cde3a45ee4b9cddee75897739dbc196d6396e117cb1ce28e1ad0/src/watcher/tcp.zig\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst assert = std.debug.assert;\nconst posix = std.posix;\nconst stream = @import(\"stream.zig\");\nconst common = @import(\"common.zig\");\n\n/// TCP client and server.\n///\n/// This is a \"higher-level abstraction\" in libxev. The goal of higher-level\n/// abstractions in libxev are to make it easier to use specific functionality\n/// with the event loop, but does not promise perfect flexibility or optimal\n/// performance. In almost all cases, the abstraction is good enough. But,\n/// if you have specific needs or want to push for the most optimal performance,\n/// use the platform-specific Loop directly.\npub fn TCP(comptime xev: type) type {\n    return struct {\n        const Self = @This();\n        const FdType = if (xev.backend == .iocp) std.os.windows.HANDLE else posix.socket_t;\n\n        fd: FdType,\n\n        pub usingnamespace stream.Stream(xev, Self, .{\n            .close = true,\n            .read = .recv,\n            .write = .send,\n        });\n\n        /// Initialize a new TCP with the family from the given address. Only\n        /// the family is used, the actual address has no impact on the created\n        /// resource.\n        pub fn init(addr: std.net.Address) !Self {\n            if (xev.backend == .wasi_poll) @compileError(\"unsupported in WASI\");\n\n            const fd = if (xev.backend == .iocp)\n                try std.os.windows.WSASocketW(addr.any.family, posix.SOCK.STREAM, 0, null, 0, std.os.windows.ws2_32.WSA_FLAG_OVERLAPPED)\n            else fd: {\n                // On io_uring we don't use non-blocking sockets because we may\n                // just get EAGAIN over and over from completions.\n                const flags = flags: {\n                    var flags: u32 = posix.SOCK.STREAM | posix.SOCK.CLOEXEC;\n                    if (xev.backend != .io_uring) flags |= posix.SOCK.NONBLOCK;\n                    break :flags flags;\n                };\n                break :fd try posix.socket(addr.any.family, flags, 0);\n            };\n\n            return .{\n                .fd = fd,\n            };\n        }\n\n        /// Initialize a TCP socket from a file descriptor.\n        pub fn initFd(fd: FdType) Self {\n            return .{\n                .fd = fd,\n            };\n        }\n\n        /// Bind the address to the socket.\n        pub fn bind(self: Self, addr: std.net.Address) !void {\n            if (xev.backend == .wasi_poll) @compileError(\"unsupported in WASI\");\n\n            const fd = if (xev.backend == .iocp) @as(std.os.windows.ws2_32.SOCKET, @ptrCast(self.fd)) else self.fd;\n\n            try posix.setsockopt(fd, posix.SOL.SOCKET, posix.SO.REUSEADDR, &std.mem.toBytes(@as(c_int, 1)));\n            try posix.bind(fd, &addr.any, addr.getOsSockLen());\n        }\n\n        /// Listen for connections on the socket. This puts the socket into passive\n        /// listening mode. Connections must still be accepted one at a time.\n        pub fn listen(self: Self, backlog: u31) !void {\n            if (xev.backend == .wasi_poll) @compileError(\"unsupported in WASI\");\n\n            const fd = if (xev.backend == .iocp) @as(std.os.windows.ws2_32.SOCKET, @ptrCast(self.fd)) else self.fd;\n\n            try posix.listen(fd, backlog);\n        }\n\n        /// Accept a single connection.\n        pub fn accept(\n            self: Self,\n            loop: *xev.Loop,\n            c: *xev.Completion,\n            comptime Userdata: type,\n            userdata: ?*Userdata,\n            comptime cb: *const fn (\n                ud: ?*Userdata,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: AcceptError!Self,\n            ) xev.CallbackAction,\n        ) void {\n            c.* = .{\n                .op = .{\n                    .accept = .{\n                        .socket = self.fd,\n                    },\n                },\n\n                .userdata = userdata,\n                .callback = (struct {\n                    fn callback(\n                        ud: ?*anyopaque,\n                        l_inner: *xev.Loop,\n                        c_inner: *xev.Completion,\n                        r: xev.Result,\n                    ) xev.CallbackAction {\n                        return @call(.always_inline, cb, .{\n                            common.userdataValue(Userdata, ud),\n                            l_inner,\n                            c_inner,\n                            if (r.accept) |fd| initFd(fd) else |err| err,\n                        });\n                    }\n                }).callback,\n            };\n\n            // If we're dup-ing, then we ask the backend to manage the fd.\n            switch (xev.backend) {\n                .io_uring,\n                .kqueue,\n                .wasi_poll,\n                .iocp,\n                => {},\n\n                .epoll => c.flags.dup = true,\n            }\n\n            loop.add(c);\n        }\n\n        /// Establish a connection as a client.\n        pub fn connect(\n            self: Self,\n            loop: *xev.Loop,\n            c: *xev.Completion,\n            addr: std.net.Address,\n            comptime Userdata: type,\n            userdata: ?*Userdata,\n            comptime cb: *const fn (\n                ud: ?*Userdata,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                s: Self,\n                r: ConnectError!void,\n            ) xev.CallbackAction,\n        ) void {\n            if (xev.backend == .wasi_poll) @compileError(\"unsupported in WASI\");\n\n            c.* = .{\n                .op = .{\n                    .connect = .{\n                        .socket = self.fd,\n                        .addr = addr,\n                    },\n                },\n\n                .userdata = userdata,\n                .callback = (struct {\n                    fn callback(\n                        ud: ?*anyopaque,\n                        l_inner: *xev.Loop,\n                        c_inner: *xev.Completion,\n                        r: xev.Result,\n                    ) xev.CallbackAction {\n                        return @call(.always_inline, cb, .{\n                            common.userdataValue(Userdata, ud),\n                            l_inner,\n                            c_inner,\n                            initFd(c_inner.op.connect.socket),\n                            if (r.connect) |_| {} else |err| err,\n                        });\n                    }\n                }).callback,\n            };\n\n            loop.add(c);\n        }\n\n        /// Shutdown the socket. This always only shuts down the writer side. You\n        /// can use the lower level interface directly to control this if the\n        /// platform supports it.\n        pub fn shutdown(\n            self: Self,\n            loop: *xev.Loop,\n            c: *xev.Completion,\n            comptime Userdata: type,\n            userdata: ?*Userdata,\n            comptime cb: *const fn (\n                ud: ?*Userdata,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                s: Self,\n                r: ShutdownError!void,\n            ) xev.CallbackAction,\n        ) void {\n            c.* = .{\n                .op = .{\n                    .shutdown = .{\n                        .socket = self.fd,\n                        .how = .send,\n                    },\n                },\n                .userdata = userdata,\n                .callback = (struct {\n                    fn callback(\n                        ud: ?*anyopaque,\n                        l_inner: *xev.Loop,\n                        c_inner: *xev.Completion,\n                        r: xev.Result,\n                    ) xev.CallbackAction {\n                        return @call(.always_inline, cb, .{\n                            common.userdataValue(Userdata, ud),\n                            l_inner,\n                            c_inner,\n                            initFd(c_inner.op.shutdown.socket),\n                            if (r.shutdown) |_| {} else |err| err,\n                        });\n                    }\n                }).callback,\n            };\n\n            loop.add(c);\n        }\n\n        pub const AcceptError = xev.AcceptError;\n        pub const ConnectError = xev.ConnectError;\n        pub const ShutdownError = xev.ShutdownError;\n\n        test \"TCP: accept/connect/send/recv/close\" {\n            // We have no way to get a socket in WASI from a WASI context.\n            if (xev.backend == .wasi_poll) return error.SkipZigTest;\n\n            const testing = std.testing;\n\n            var loop = try xev.Loop.init(.{});\n            defer loop.deinit();\n\n            // Choose random available port (Zig #14907)\n            var address = try std.net.Address.parseIp4(\"127.0.0.1\", 0);\n            const server = try Self.init(address);\n\n            // Bind and listen\n            try server.bind(address);\n            try server.listen(1);\n\n            // Retrieve bound port and initialize client\n            var sock_len = address.getOsSockLen();\n            const fd = if (xev.backend == .iocp) @as(std.os.windows.ws2_32.SOCKET, @ptrCast(server.fd)) else server.fd;\n            try posix.getsockname(fd, &address.any, &sock_len);\n            const client = try Self.init(address);\n\n            //const address = try std.net.Address.parseIp4(\"127.0.0.1\", 3132);\n            //var server = try Self.init(address);\n            //var client = try Self.init(address);\n\n            // Completions we need\n            var c_accept: xev.Completion = undefined;\n            var c_connect: xev.Completion = undefined;\n\n            // Accept\n            var server_conn: ?Self = null;\n            server.accept(&loop, &c_accept, ?Self, &server_conn, (struct {\n                fn callback(\n                    ud: ?*?Self,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    r: AcceptError!Self,\n                ) xev.CallbackAction {\n                    ud.?.* = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Connect\n            var connected: bool = false;\n            client.connect(&loop, &c_connect, address, bool, &connected, (struct {\n                fn callback(\n                    ud: ?*bool,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: ConnectError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = true;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Wait for the connection to be established\n            try loop.run(.until_done);\n            try testing.expect(server_conn != null);\n            try testing.expect(connected);\n\n            // Close the server\n            var server_closed = false;\n            server.close(&loop, &c_accept, bool, &server_closed, (struct {\n                fn callback(\n                    ud: ?*bool,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: Self.CloseError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = true;\n                    return .disarm;\n                }\n            }).callback);\n            try loop.run(.until_done);\n            try testing.expect(server_closed);\n\n            // Send\n            var send_buf = [_]u8{ 1, 1, 2, 3, 5, 8, 13 };\n            client.write(&loop, &c_connect, .{ .slice = &send_buf }, void, null, (struct {\n                fn callback(\n                    _: ?*void,\n                    _: *xev.Loop,\n                    c: *xev.Completion,\n                    _: Self,\n                    _: xev.WriteBuffer,\n                    r: Self.WriteError!usize,\n                ) xev.CallbackAction {\n                    _ = c;\n                    _ = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Receive\n            var recv_buf: [128]u8 = undefined;\n            var recv_len: usize = 0;\n            server_conn.?.read(&loop, &c_accept, .{ .slice = &recv_buf }, usize, &recv_len, (struct {\n                fn callback(\n                    ud: ?*usize,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    _: xev.ReadBuffer,\n                    r: Self.ReadError!usize,\n                ) xev.CallbackAction {\n                    ud.?.* = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Wait for the send/receive\n            try loop.run(.until_done);\n            try testing.expectEqualSlices(u8, &send_buf, recv_buf[0..recv_len]);\n\n            // Close\n            server_conn.?.close(&loop, &c_accept, ?Self, &server_conn, (struct {\n                fn callback(\n                    ud: ?*?Self,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: Self.CloseError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = null;\n                    return .disarm;\n                }\n            }).callback);\n            client.close(&loop, &c_connect, bool, &connected, (struct {\n                fn callback(\n                    ud: ?*bool,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: Self.CloseError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = false;\n                    return .disarm;\n                }\n            }).callback);\n\n            try loop.run(.until_done);\n            try testing.expect(server_conn == null);\n            try testing.expect(!connected);\n            try testing.expect(server_closed);\n        }\n\n        // Potentially flaky - this test could hang if the sender is unable to\n        // write everything to the socket for whatever reason\n        // (e.g. incorrectly sized buffer on the receiver side), or if the\n        // receiver is trying to receive while sender has nothing left to send.\n        //\n        // Overview:\n        // 1. Set up server and client sockets\n        // 2. connect & accept, set SO_SNDBUF to 8kB on the client\n        // 3. Try to send 1MB buffer from client to server without queuing, this _should_ fail\n        //    and theoretically send <= 8kB, but in practice, it seems to write ~32kB.\n        //    Asserts that <= 100kB was written\n        // 4. Set up a queued write with the remaining buffer, shutdown() the socket afterwards\n        // 5. Set up a receiver that loops until it receives the entire buffer\n        // 6. Assert send_buf == recv_buf\n        test \"TCP: Queued writes\" {\n            // We have no way to get a socket in WASI from a WASI context.\n            if (xev.backend == .wasi_poll) return error.SkipZigTest;\n            // Windows doesn't seem to respect the SNDBUF socket option.\n            if (builtin.os.tag == .windows) return error.SkipZigTest;\n\n            const testing = std.testing;\n\n            var loop = try xev.Loop.init(.{});\n            defer loop.deinit();\n\n            // Choose random available port (Zig #14907)\n            var address = try std.net.Address.parseIp4(\"127.0.0.1\", 0);\n            const server = try Self.init(address);\n\n            // Bind and listen\n            try server.bind(address);\n            try server.listen(1);\n\n            // Retrieve bound port and initialize client\n            var sock_len = address.getOsSockLen();\n            try posix.getsockname(server.fd, &address.any, &sock_len);\n            const client = try Self.init(address);\n\n            // Completions we need\n            var c_accept: xev.Completion = undefined;\n            var c_connect: xev.Completion = undefined;\n\n            // Accept\n            var server_conn: ?Self = null;\n            server.accept(&loop, &c_accept, ?Self, &server_conn, (struct {\n                fn callback(\n                    ud: ?*?Self,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    r: AcceptError!Self,\n                ) xev.CallbackAction {\n                    ud.?.* = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Connect\n            var connected: bool = false;\n            client.connect(&loop, &c_connect, address, bool, &connected, (struct {\n                fn callback(\n                    ud: ?*bool,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: ConnectError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = true;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Wait for the connection to be established\n            try loop.run(.until_done);\n            try testing.expect(server_conn != null);\n            try testing.expect(connected);\n\n            // Close the server\n            var server_closed = false;\n            server.close(&loop, &c_accept, bool, &server_closed, (struct {\n                fn callback(\n                    ud: ?*bool,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: Self.CloseError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = true;\n                    return .disarm;\n                }\n            }).callback);\n            try loop.run(.until_done);\n            try testing.expect(server_closed);\n\n            // Unqueued send - Limit send buffer to 8kB, this should force partial writes.\n            try posix.setsockopt(client.fd, posix.SOL.SOCKET, posix.SO.SNDBUF, &std.mem.toBytes(@as(c_int, 8192)));\n\n            const send_buf = [_]u8{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 0 } ** 100_000;\n            var sent_unqueued: usize = 0;\n\n            // First we try to send the whole 1MB buffer in one write operation, this _should_ result\n            // in a partial write.\n            client.write(&loop, &c_connect, .{ .slice = &send_buf }, usize, &sent_unqueued, (struct {\n                fn callback(\n                    sent_unqueued_inner: ?*usize,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    _: xev.WriteBuffer,\n                    r: Self.WriteError!usize,\n                ) xev.CallbackAction {\n                    sent_unqueued_inner.?.* = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Make sure that we sent a small fraction of the buffer\n            try loop.run(.until_done);\n            // SO_SNDBUF doesn't seem to be respected exactly, sent_unqueued will often be ~32kB\n            // even though SO_SNDBUF was set to 8kB\n            try testing.expect(sent_unqueued < (send_buf.len / 10));\n\n            // Set up queued write\n            var w_queue = Self.WriteQueue{};\n            var wr_send: xev.TCP.WriteRequest = undefined;\n            var sent_queued: usize = 0;\n            const queued_slice = send_buf[sent_unqueued..];\n            client.queueWrite(&loop, &w_queue, &wr_send, .{ .slice = queued_slice }, usize, &sent_queued, (struct {\n                fn callback(\n                    sent_queued_inner: ?*usize,\n                    l: *xev.Loop,\n                    c: *xev.Completion,\n                    tcp: Self,\n                    _: xev.WriteBuffer,\n                    r: Self.WriteError!usize,\n                ) xev.CallbackAction {\n                    sent_queued_inner.?.* = r catch unreachable;\n\n                    tcp.shutdown(l, c, void, null, (struct {\n                        fn callback(\n                            _: ?*void,\n                            _: *xev.Loop,\n                            _: *xev.Completion,\n                            _: Self,\n                            _: Self.ShutdownError!void,\n                        ) xev.CallbackAction {\n                            return .disarm;\n                        }\n                    }).callback);\n\n                    return .disarm;\n                }\n            }).callback);\n\n            // Set up receiver which is going to keep reading until it reads the full\n            // send buffer\n            const Receiver = struct {\n                loop: *xev.Loop,\n                conn: Self,\n                completion: xev.Completion = .{},\n                buf: [send_buf.len]u8 = undefined,\n                bytes_read: usize = 0,\n\n                pub fn read(receiver: *@This()) void {\n                    if (receiver.bytes_read == receiver.buf.len) return;\n\n                    const read_buf = xev.ReadBuffer{\n                        .slice = receiver.buf[receiver.bytes_read..],\n                    };\n                    receiver.conn.read(receiver.loop, &receiver.completion, read_buf, @This(), receiver, readCb);\n                }\n\n                pub fn readCb(\n                    receiver_opt: ?*@This(),\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    _: xev.ReadBuffer,\n                    r: Self.ReadError!usize,\n                ) xev.CallbackAction {\n                    var receiver = receiver_opt.?;\n                    const n_bytes = r catch unreachable;\n\n                    receiver.bytes_read += n_bytes;\n                    if (receiver.bytes_read < send_buf.len) {\n                        receiver.read();\n                    }\n\n                    return .disarm;\n                }\n            };\n            var receiver = Receiver{\n                .loop = &loop,\n                .conn = server_conn.?,\n            };\n            receiver.read();\n\n            // Wait for the send/receive\n            try loop.run(.until_done);\n            try testing.expectEqualSlices(u8, &send_buf, receiver.buf[0..receiver.bytes_read]);\n            try testing.expect(send_buf.len == sent_unqueued + sent_queued);\n\n            // Close\n            server_conn.?.close(&loop, &c_accept, ?Self, &server_conn, (struct {\n                fn callback(\n                    ud: ?*?Self,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: Self.CloseError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = null;\n                    return .disarm;\n                }\n            }).callback);\n            client.close(&loop, &c_connect, bool, &connected, (struct {\n                fn callback(\n                    ud: ?*bool,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: Self.CloseError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = false;\n                    return .disarm;\n                }\n            }).callback);\n\n            try loop.run(.until_done);\n            try testing.expect(server_conn == null);\n            try testing.expect(!connected);\n            try testing.expect(server_closed);\n        }\n    };\n}\n```\n\n```zig /home/msharran/.cache/zig/p/12205b2b47fe61a4cde3a45ee4b9cddee75897739dbc196d6396e117cb1ce28e1ad0/src/watcher/stream.zig\nconst std = @import(\"std\");\nconst assert = std.debug.assert;\nconst builtin = @import(\"builtin\");\nconst common = @import(\"common.zig\");\nconst queue = @import(\"../queue.zig\");\n\n/// Options for creating a stream type. Each of the options makes the\n/// functionality available for the stream.\npub const Options = struct {\n    read: ReadMethod,\n    write: WriteMethod,\n    close: bool,\n\n    /// True to schedule the read/write on the threadpool.\n    threadpool: bool = false,\n\n    pub const ReadMethod = enum { none, read, recv };\n    pub const WriteMethod = enum { none, write, send };\n};\n\n/// Creates a stream type that is meant to be embedded within other\n/// types using \"usingnamespace\". A stream is something that supports read,\n/// write, close, etc. The exact operations supported are defined by the\n/// \"options\" struct.\n///\n/// T requirements:\n///   - field named \"fd\" of type fd_t or socket_t\n///   - decl named \"initFd\" to initialize a new T from a fd\n///\npub fn Stream(comptime xev: type, comptime T: type, comptime options: Options) type {\n    return struct {\n        pub usingnamespace if (options.close) Closeable(xev, T, options) else struct {};\n        pub usingnamespace if (options.read != .none) Readable(xev, T, options) else struct {};\n        pub usingnamespace if (options.write != .none) Writeable(xev, T, options) else struct {};\n    };\n}\n\npub fn Closeable(comptime xev: type, comptime T: type, comptime options: Options) type {\n    _ = options;\n\n    return struct {\n        const Self = T;\n\n        pub const CloseError = xev.CloseError;\n\n        /// Close the socket.\n        pub fn close(\n            self: Self,\n            loop: *xev.Loop,\n            c: *xev.Completion,\n            comptime Userdata: type,\n            userdata: ?*Userdata,\n            comptime cb: *const fn (\n                ud: ?*Userdata,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                s: Self,\n                r: CloseError!void,\n            ) xev.CallbackAction,\n        ) void {\n            c.* = .{\n                .op = .{ .close = .{ .fd = self.fd } },\n                .userdata = userdata,\n                .callback = (struct {\n                    fn callback(\n                        ud: ?*anyopaque,\n                        l_inner: *xev.Loop,\n                        c_inner: *xev.Completion,\n                        r: xev.Result,\n                    ) xev.CallbackAction {\n                        const fd = T.initFd(c_inner.op.close.fd);\n                        return @call(.always_inline, cb, .{\n                            common.userdataValue(Userdata, ud),\n                            l_inner,\n                            c_inner,\n                            fd,\n                            if (r.close) |_| {} else |err| err,\n                        });\n                    }\n                }).callback,\n            };\n\n            loop.add(c);\n        }\n    };\n}\n\npub fn Readable(comptime xev: type, comptime T: type, comptime options: Options) type {\n    return struct {\n        const Self = T;\n\n        pub const ReadError = xev.ReadError;\n\n        /// Read from the socket. This performs a single read. The callback must\n        /// requeue the read if additional reads want to be performed. Additional\n        /// reads simultaneously can be queued by calling this multiple times. Note\n        /// that depending on the backend, the reads can happen out of order.\n        pub fn read(\n            self: Self,\n            loop: *xev.Loop,\n            c: *xev.Completion,\n            buf: xev.ReadBuffer,\n            comptime Userdata: type,\n            userdata: ?*Userdata,\n            comptime cb: *const fn (\n                ud: ?*Userdata,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                s: Self,\n                b: xev.ReadBuffer,\n                r: ReadError!usize,\n            ) xev.CallbackAction,\n        ) void {\n            switch (buf) {\n                inline .slice, .array => {\n                    c.* = .{\n                        .op = switch (options.read) {\n                            .none => unreachable,\n\n                            .read => .{\n                                .read = .{\n                                    .fd = self.fd,\n                                    .buffer = buf,\n                                },\n                            },\n\n                            .recv => .{\n                                .recv = .{\n                                    .fd = self.fd,\n                                    .buffer = buf,\n                                },\n                            },\n                        },\n                        .userdata = userdata,\n                        .callback = (struct {\n                            fn callback(\n                                ud: ?*anyopaque,\n                                l_inner: *xev.Loop,\n                                c_inner: *xev.Completion,\n                                r: xev.Result,\n                            ) xev.CallbackAction {\n                                return switch (options.read) {\n                                    .none => unreachable,\n\n                                    .recv => @call(.always_inline, cb, .{\n                                        common.userdataValue(Userdata, ud),\n                                        l_inner,\n                                        c_inner,\n                                        T.initFd(c_inner.op.recv.fd),\n                                        c_inner.op.recv.buffer,\n                                        if (r.recv) |v| v else |err| err,\n                                    }),\n\n                                    .read => @call(.always_inline, cb, .{\n                                        common.userdataValue(Userdata, ud),\n                                        l_inner,\n                                        c_inner,\n                                        T.initFd(c_inner.op.read.fd),\n                                        c_inner.op.read.buffer,\n                                        if (r.read) |v| v else |err| err,\n                                    }),\n                                };\n                            }\n                        }).callback,\n                    };\n\n                    // If we're dup-ing, then we ask the backend to manage the fd.\n                    switch (xev.backend) {\n                        .io_uring,\n                        .wasi_poll,\n                        .iocp,\n                        => {},\n\n                        .epoll => {\n                            if (options.threadpool)\n                                c.flags.threadpool = true\n                            else\n                                c.flags.dup = true;\n                        },\n\n                        .kqueue => {\n                            if (options.threadpool) c.flags.threadpool = true;\n                        },\n                    }\n\n                    loop.add(c);\n                },\n            }\n        }\n    };\n}\n\npub fn Writeable(comptime xev: type, comptime T: type, comptime options: Options) type {\n    return struct {\n        const Self = T;\n\n        pub const WriteError = xev.WriteError;\n\n        /// WriteQueue is the queue of write requests for ordered writes.\n        /// This can be copied around.\n        pub const WriteQueue = queue.Intrusive(WriteRequest);\n\n        /// WriteRequest is a single request for a write. It wraps a\n        /// completion so that it can be inserted into the WriteQueue.\n        pub const WriteRequest = struct {\n            completion: xev.Completion = .{},\n            userdata: ?*anyopaque = null,\n\n            /// This is the original buffer passed to queueWrite. We have\n            /// to keep track of this because we may be forced to split\n            /// the write or rearm the write due to partial writes, but when\n            /// we call the final callback we want to pass the original\n            /// complete buffer.\n            full_write_buffer: xev.WriteBuffer,\n\n            next: ?*@This() = null,\n\n            /// This can be used to convert a completion pointer back to\n            /// a WriteRequest. This is only safe of course if the completion\n            /// originally is from a write request. This is useful for getting\n            /// the WriteRequest back in a callback from queuedWrite.\n            pub fn from(c: *xev.Completion) *WriteRequest {\n                return @fieldParentPtr(\"completion\", c);\n            }\n        };\n\n        /// Write to the stream. This queues the writes to ensure they\n        /// remain in order. Queueing has a small overhead: you must\n        /// maintain a WriteQueue and WriteRequests instead of just\n        /// Completions.\n        ///\n        /// If ordering isn't important, or you can maintain ordering\n        /// naturally in your program, consider using write since it\n        /// has a slightly smaller overhead.\n        ///\n        /// The \"CallbackAction\" return value of this callback behaves slightly\n        /// different. The \"rearm\" return value will re-queue the same write\n        /// at the end of the queue.\n        ///\n        /// It is safe to call this at anytime from the main thread.\n        pub fn queueWrite(\n            self: Self,\n            loop: *xev.Loop,\n            q: *WriteQueue,\n            req: *WriteRequest,\n            buf: xev.WriteBuffer,\n            comptime Userdata: type,\n            userdata: ?*Userdata,\n            comptime cb: *const fn (\n                ud: ?*Userdata,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                s: Self,\n                b: xev.WriteBuffer,\n                r: WriteError!usize,\n            ) xev.CallbackAction,\n        ) void {\n            // Initialize our completion\n            req.* = .{ .full_write_buffer = buf };\n            // Must be kept in sync with partial write logic inside the callback\n            self.write_init(&req.completion, buf);\n            req.completion.userdata = q;\n            req.completion.callback = (struct {\n                fn callback(\n                    ud: ?*anyopaque,\n                    l_inner: *xev.Loop,\n                    c_inner: *xev.Completion,\n                    r: xev.Result,\n                ) xev.CallbackAction {\n                    const q_inner = @as(?*WriteQueue, @ptrCast(@alignCast(ud))).?;\n\n                    // The queue MUST have a request because a completion\n                    // can only be added if the queue is not empty, and\n                    // nothing else should be popping!.\n                    //\n                    // We only peek the request here (not pop) because we may\n                    // need to rearm this write if the write was partial.\n                    const req_inner: *WriteRequest = q_inner.head.?;\n\n                    const cb_res = write_result(c_inner, r);\n                    var result: WriteError!usize = cb_res.result;\n\n                    // Checks whether the entire buffer was written, this is\n                    // necessary to guarantee correct ordering of writes.\n                    // If the write was partial, it re-submits the remainder of\n                    // the buffer.\n                    const queued_len = writeBufferLength(cb_res.buf);\n                    if (cb_res.result) |written_len| {\n                        if (written_len < queued_len) {\n                            // Write remainder of the buffer, reusing the same completion\n                            const rem_buf = writeBufferRemainder(cb_res.buf, written_len);\n                            cb_res.writer.write_init(&req_inner.completion, rem_buf);\n                            req_inner.completion.userdata = q_inner;\n                            req_inner.completion.callback = callback;\n                            l_inner.add(&req_inner.completion);\n                            return .disarm;\n                        }\n\n                        // We wrote the entire buffer, modify the result to indicate\n                        // to the caller that all bytes have been written.\n                        result = writeBufferLength(req_inner.full_write_buffer);\n                    } else |_| {}\n\n                    // We can pop previously peeked request.\n                    _ = q_inner.pop().?;\n\n                    const action = @call(.always_inline, cb, .{\n                        common.userdataValue(Userdata, req_inner.userdata),\n                        l_inner,\n                        c_inner,\n                        cb_res.writer,\n                        req_inner.full_write_buffer,\n                        result,\n                    });\n\n                    // Rearm requeues this request, it doesn't return rearm\n                    // on the actual callback here...\n                    if (action == .rearm) q_inner.push(req_inner);\n\n                    // If we have another request, add that completion next.\n                    if (q_inner.head) |req_next| l_inner.add(&req_next.completion);\n\n                    // We always disarm because the completion in the next\n                    // request will be used if there is more to queue.\n                    return .disarm;\n                }\n            }).callback;\n\n            // The userdata as to go on the WriteRequest because we need\n            // our actual completion userdata to be the WriteQueue so that\n            // we can process the queue.\n            req.userdata = @as(?*anyopaque, @ptrCast(@alignCast(userdata)));\n\n            // If the queue is empty, then we add our completion. Otherwise,\n            // the previously queued writes will trigger this one.\n            if (q.empty()) loop.add(&req.completion);\n\n            // We always add this item to our queue no matter what\n            q.push(req);\n        }\n\n        /// Write to the stream. This performs a single write. Additional\n        /// writes can be requested by calling this multiple times.\n        ///\n        /// IMPORTANT: writes are NOT queued. There is no order guarantee\n        /// if this is called multiple times. If ordered writes are important\n        /// (they usually are!) then you should only call write again once\n        /// the previous write callback is called.\n        ///\n        /// If ordering is important, use queueWrite instead.\n        pub fn write(\n            self: Self,\n            loop: *xev.Loop,\n            c: *xev.Completion,\n            buf: xev.WriteBuffer,\n            comptime Userdata: type,\n            userdata: ?*Userdata,\n            comptime cb: *const fn (\n                ud: ?*Userdata,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                s: Self,\n                b: xev.WriteBuffer,\n                r: WriteError!usize,\n            ) xev.CallbackAction,\n        ) void {\n            self.write_init(c, buf);\n            c.userdata = userdata;\n            c.callback = (struct {\n                fn callback(\n                    ud: ?*anyopaque,\n                    l_inner: *xev.Loop,\n                    c_inner: *xev.Completion,\n                    r: xev.Result,\n                ) xev.CallbackAction {\n                    const cb_res = write_result(c_inner, r);\n                    return @call(.always_inline, cb, .{\n                        common.userdataValue(Userdata, ud),\n                        l_inner,\n                        c_inner,\n                        cb_res.writer,\n                        cb_res.buf,\n                        cb_res.result,\n                    });\n                }\n            }).callback;\n\n            loop.add(c);\n        }\n\n        /// Extracts the result from a completion for a write callback.\n        inline fn write_result(c: *xev.Completion, r: xev.Result) struct {\n            writer: Self,\n            buf: xev.WriteBuffer,\n            result: WriteError!usize,\n        } {\n            return switch (options.write) {\n                .none => unreachable,\n\n                .send => .{\n                    .writer = T.initFd(c.op.send.fd),\n                    .buf = c.op.send.buffer,\n                    .result = if (r.send) |v| v else |err| err,\n                },\n\n                .write => .{\n                    .writer = T.initFd(c.op.write.fd),\n                    .buf = c.op.write.buffer,\n                    .result = if (r.write) |v| v else |err| err,\n                },\n            };\n        }\n\n        /// Initialize the completion c for a write. This does NOT set\n        /// userdata or a callback.\n        fn write_init(\n            self: Self,\n            c: *xev.Completion,\n            buf: xev.WriteBuffer,\n        ) void {\n            switch (buf) {\n                inline .slice, .array => {\n                    c.* = .{\n                        .op = switch (options.write) {\n                            .none => unreachable,\n\n                            .write => .{\n                                .write = .{\n                                    .fd = self.fd,\n                                    .buffer = buf,\n                                },\n                            },\n\n                            .send => .{\n                                .send = .{\n                                    .fd = self.fd,\n                                    .buffer = buf,\n                                },\n                            },\n                        },\n                    };\n\n                    // If we're dup-ing, then we ask the backend to manage the fd.\n                    switch (xev.backend) {\n                        .io_uring,\n                        .wasi_poll,\n                        .iocp,\n                        => {},\n\n                        .epoll => {\n                            if (options.threadpool) {\n                                c.flags.threadpool = true;\n                            } else {\n                                c.flags.dup = true;\n                            }\n                        },\n\n                        .kqueue => {\n                            if (options.threadpool) c.flags.threadpool = true;\n                        },\n                    }\n                },\n            }\n        }\n\n        /// Returns the length of the write buffer\n        fn writeBufferLength(buf: xev.WriteBuffer) usize {\n            return switch (buf) {\n                .slice => |slice| slice.len,\n                .array => |array| array.len,\n            };\n        }\n\n        /// Given a `WriteBuffer` and number of bytes written during the previous\n        /// write operation, returns a new `WriteBuffer` with remaining data.\n        fn writeBufferRemainder(buf: xev.WriteBuffer, offset: usize) xev.WriteBuffer {\n            switch (buf) {\n                .slice => |slice| {\n                    assert(offset <= slice.len);\n                    return .{ .slice = slice[offset..] };\n                },\n                .array => |array| {\n                    assert(offset <= array.len);\n                    const rem_len = array.len - offset;\n                    var wb = xev.WriteBuffer{ .array = .{\n                        .array = undefined,\n                        .len = rem_len,\n                    } };\n                    @memcpy(\n                        wb.array.array[0..rem_len],\n                        array.array[offset..][0..rem_len],\n                    );\n                    return wb;\n                },\n            }\n        }\n    };\n}\n\n/// Creates a generic stream type that supports read, write, close. This\n/// can be used for any file descriptor that would exhibit normal blocking\n/// behavior on read/write. This should NOT be used for local files because\n/// local files have some special properties; you should use xev.File for that.\npub fn GenericStream(comptime xev: type) type {\n    return struct {\n        const Self = @This();\n\n        /// The underlying file\n        fd: std.posix.fd_t,\n\n        pub usingnamespace Stream(xev, Self, .{\n            .close = true,\n            .read = .read,\n            .write = .write,\n        });\n\n        /// Initialize a generic stream from a file descriptor.\n        pub fn initFd(fd: std.posix.fd_t) Self {\n            return .{\n                .fd = fd,\n            };\n        }\n\n        /// Clean up any watcher resources. This does NOT close the file.\n        /// If you want to close the file you must call close or do so\n        /// synchronously.\n        pub fn deinit(self: *const Self) void {\n            _ = self;\n        }\n\n        test \"pty: child to parent\" {\n            const testing = std.testing;\n            switch (builtin.os.tag) {\n                .linux, .macos => {},\n                else => return error.SkipZigTest,\n            }\n\n            // Create the pty parent/child side.\n            var pty = try Pty.init();\n            defer pty.deinit();\n\n            var loop = try xev.Loop.init(.{});\n            defer loop.deinit();\n\n            const parent = initFd(pty.parent);\n            const child = initFd(pty.child);\n\n            // Read\n            var read_buf: [128]u8 = undefined;\n            var read_len: ?usize = null;\n            var c_read: xev.Completion = undefined;\n            parent.read(&loop, &c_read, .{ .slice = &read_buf }, ?usize, &read_len, (struct {\n                fn callback(\n                    ud: ?*?usize,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    _: xev.ReadBuffer,\n                    r: Self.ReadError!usize,\n                ) xev.CallbackAction {\n                    ud.?.* = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // This should not block!\n            try loop.run(.no_wait);\n            try testing.expect(read_len == null);\n\n            // Send\n            const send_buf = \"hello, world!\";\n            var c_write: xev.Completion = undefined;\n            child.write(&loop, &c_write, .{ .slice = send_buf }, void, null, (struct {\n                fn callback(\n                    _: ?*void,\n                    _: *xev.Loop,\n                    c: *xev.Completion,\n                    _: Self,\n                    _: xev.WriteBuffer,\n                    r: Self.WriteError!usize,\n                ) xev.CallbackAction {\n                    _ = c;\n                    _ = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // The write and read should trigger\n            try loop.run(.until_done);\n            try testing.expect(read_len != null);\n            try testing.expectEqualSlices(u8, send_buf, read_buf[0..read_len.?]);\n        }\n\n        test \"pty: parent to child\" {\n            const testing = std.testing;\n            switch (builtin.os.tag) {\n                .linux, .macos => {},\n                else => return error.SkipZigTest,\n            }\n\n            // Create the pty parent/child side.\n            var pty = try Pty.init();\n            defer pty.deinit();\n\n            var loop = try xev.Loop.init(.{});\n            defer loop.deinit();\n\n            const parent = initFd(pty.parent);\n            const child = initFd(pty.child);\n\n            // Read\n            var read_buf: [128]u8 = undefined;\n            var read_len: ?usize = null;\n            var c_read: xev.Completion = undefined;\n            child.read(&loop, &c_read, .{ .slice = &read_buf }, ?usize, &read_len, (struct {\n                fn callback(\n                    ud: ?*?usize,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    _: xev.ReadBuffer,\n                    r: Self.ReadError!usize,\n                ) xev.CallbackAction {\n                    ud.?.* = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // This should not block!\n            try loop.run(.no_wait);\n            try testing.expect(read_len == null);\n\n            // Send (note the newline at the end of the buf is important\n            // since we're in cooked mode)\n            const send_buf = \"hello, world!\\n\";\n            var c_write: xev.Completion = undefined;\n            parent.write(&loop, &c_write, .{ .slice = send_buf }, void, null, (struct {\n                fn callback(\n                    _: ?*void,\n                    _: *xev.Loop,\n                    c: *xev.Completion,\n                    _: Self,\n                    _: xev.WriteBuffer,\n                    r: Self.WriteError!usize,\n                ) xev.CallbackAction {\n                    _ = c;\n                    _ = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // The write and read should trigger\n            try loop.run(.until_done);\n            try testing.expect(read_len != null);\n            try testing.expectEqualSlices(u8, send_buf, read_buf[0..read_len.?]);\n        }\n\n        test \"pty: queued writes\" {\n            const testing = std.testing;\n            switch (builtin.os.tag) {\n                .linux, .macos => {},\n                else => return error.SkipZigTest,\n            }\n\n            // Create the pty parent/child side.\n            var pty = try Pty.init();\n            defer pty.deinit();\n\n            var loop = try xev.Loop.init(.{});\n            defer loop.deinit();\n\n            const parent = initFd(pty.parent);\n            const child = initFd(pty.child);\n\n            // Read\n            var read_buf: [128]u8 = undefined;\n            var read_len: ?usize = null;\n            var c_read: xev.Completion = undefined;\n            child.read(&loop, &c_read, .{ .slice = &read_buf }, ?usize, &read_len, (struct {\n                fn callback(\n                    ud: ?*?usize,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    _: xev.ReadBuffer,\n                    r: Self.ReadError!usize,\n                ) xev.CallbackAction {\n                    ud.?.* = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // This should not block!\n            try loop.run(.no_wait);\n            try testing.expect(read_len == null);\n\n            var write_queue: Self.WriteQueue = .{};\n            var write_req: [2]Self.WriteRequest = undefined;\n\n            // Send (note the newline at the end of the buf is important\n            // since we're in cooked mode)\n            parent.queueWrite(\n                &loop,\n                &write_queue,\n                &write_req[0],\n                .{ .slice = \"hello, \" },\n                void,\n                null,\n                (struct {\n                    fn callback(\n                        _: ?*void,\n                        _: *xev.Loop,\n                        c: *xev.Completion,\n                        _: Self,\n                        _: xev.WriteBuffer,\n                        r: Self.WriteError!usize,\n                    ) xev.CallbackAction {\n                        _ = c;\n                        _ = r catch unreachable;\n                        return .disarm;\n                    }\n                }).callback,\n            );\n\n            var c_result: ?*xev.Completion = null;\n            parent.queueWrite(\n                &loop,\n                &write_queue,\n                &write_req[1],\n                .{ .slice = \"world!\\n\" },\n                ?*xev.Completion,\n                &c_result,\n                (struct {\n                    fn callback(\n                        ud: ?*?*xev.Completion,\n                        _: *xev.Loop,\n                        c: *xev.Completion,\n                        _: Self,\n                        _: xev.WriteBuffer,\n                        r: Self.WriteError!usize,\n                    ) xev.CallbackAction {\n                        _ = r catch unreachable;\n                        ud.?.* = c;\n                        return .disarm;\n                    }\n                }).callback,\n            );\n\n            // The write and read should trigger\n            try loop.run(.until_done);\n            try testing.expect(read_len != null);\n            try testing.expectEqualSlices(u8, \"hello, world!\\n\", read_buf[0..read_len.?]);\n\n            // Verify our completion is equal to our request\n            try testing.expect(Self.WriteRequest.from(c_result.?) == &write_req[1]);\n        }\n    };\n}\n\n/// Helper to open a pty. This isn't exposed as a public API this is only\n/// used for tests.\nconst Pty = struct {\n    /// The file descriptors for the parent/child side of the pty. This refers\n    /// to the master/slave side respectively, and while that terminology is\n    /// the officially used terminology of the syscall, I will use parent/child\n    /// here.\n    parent: std.posix.fd_t,\n    child: std.posix.fd_t,\n\n    /// Redeclare this winsize struct so we can just use a Zig struct. This\n    /// layout should be correct on all tested platforms.\n    const Winsize = extern struct {\n        ws_row: u16,\n        ws_col: u16,\n        ws_xpixel: u16,\n        ws_ypixel: u16,\n    };\n\n    // libc pty.h\n    extern \"c\" fn openpty(\n        parent: *std.posix.fd_t,\n        child: *std.posix.fd_t,\n        name: ?[*]u8,\n        termios: ?*const anyopaque, // termios but we don't use it\n        winsize: ?*const Winsize,\n    ) c_int;\n\n    pub fn init() !Pty {\n        // Reasonable size\n        var size: Winsize = .{\n            .ws_row = 80,\n            .ws_col = 80,\n            .ws_xpixel = 800,\n            .ws_ypixel = 600,\n        };\n\n        var parent_fd: std.posix.fd_t = undefined;\n        var child_fd: std.posix.fd_t = undefined;\n        if (openpty(\n            &parent_fd,\n            &child_fd,\n            null,\n            null,\n            &size,\n        ) < 0)\n            return error.OpenptyFailed;\n        errdefer {\n            _ = std.posix.system.close(parent_fd);\n            _ = std.posix.system.close(child_fd);\n        }\n\n        return .{\n            .parent = parent_fd,\n            .child = child_fd,\n        };\n    }\n\n    pub fn deinit(self: *Pty) void {\n        std.posix.close(self.parent);\n        std.posix.close(self.child);\n    }\n};\n```\n\n```zig zig-redis-server/src/command.zig\nconst std = @import(\"std\");\nconst log = std.log.scoped(.command);\nconst assert = std.debug.assert;\nconst expect = std.testing.expect;\n\nconst Message = @import(\"Resp.zig\").Message;\nconst Dictionary = @import(\"Dictionary.zig\");\npub const CommandFn = *const fn (Request) Message;\n\npub const Request = struct {\n    message: Message,\n    dict: *Dictionary,\n};\n\npub fn ping(_: Request) Message {\n    return Message.simpleString(\"PONG\");\n}\n\npub fn echo(r: Request) Message {\n    const items = r.message.value.list.items;\n    if (items.len < 2) {\n        return Message.err(\"ERR bad request: expected at least one argument\");\n    }\n\n    const value = items[1].value.single;\n    return Message.bulkString(value);\n}\n\npub fn set(r: Request) Message {\n    const items = r.message.value.list.items;\n    if (items.len < 3) {\n        return Message.err(\"ERR bad request: expected at least two arguments\");\n    }\n\n    const key = items[1].value.single;\n    const value = items[2].value.single;\n\n    r.dict.putString(key, value) catch |err| {\n        std.debug.print(\"ERR failed to set: {}\\n\", .{err});\n        return Message.err(\"ERR failed to set\");\n    };\n\n    return Message.simpleString(\"OK\");\n}\n\npub fn get(r: Request) Message {\n    const items = r.message.value.list.items;\n    if (items.len < 2) {\n        return Message.err(\"ERR bad request: expected at least two arguments\");\n    }\n\n    const key = items[1].value.single;\n\n    const value = r.dict.getString(key) catch |err| {\n        std.debug.print(\"ERR failed to set: {}\\n\", .{err});\n        return Message.err(\"ERR failed to set\");\n    };\n\n    if (value == null) {\n        return Message.nil();\n    }\n\n    return Message.bulkString(value.?);\n}\n\ntest \"set command\" {\n    var gpa = std.heap.GeneralPurposeAllocator(.{}){};\n    defer {\n        const check = gpa.deinit();\n        std.debug.print(\"mem leak check {}\\n\", .{check});\n        assert(check == .ok);\n    }\n\n    const obj_store = try Dictionary.init(gpa.allocator());\n    defer obj_store.deinit();\n\n    var msg_list = std.ArrayList(Message).init(gpa.allocator());\n    defer msg_list.deinit();\n\n    try msg_list.append(Message.bulkString(\"SET\"));\n    try msg_list.append(Message.bulkString(\"foo\"));\n    try msg_list.append(Message.bulkString(\"bar\"));\n\n    const msg = Message.initList(.Array, msg_list);\n\n    const req = Request{\n        .message = msg,\n        .dict = obj_store,\n    };\n\n    const res = set(req);\n    std.debug.print(\"res: {s}\\n\", .{res.value.single});\n\n    const got = try obj_store.getString(\"foo\");\n    try expect(got != null);\n    try std.testing.expectEqualStrings(\"bar\", got.?);\n}\n\ntest \"set command long string\" {\n    var gpa = std.heap.GeneralPurposeAllocator(.{}){};\n    defer {\n        const check = gpa.deinit();\n        std.debug.print(\"mem leak check {}\\n\", .{check});\n        assert(check == .ok);\n    }\n\n    const obj_store = try Dictionary.init(gpa.allocator());\n    defer obj_store.deinit();\n\n    var msg_list = std.ArrayList(Message).init(gpa.allocator());\n    defer msg_list.deinit();\n\n    try msg_list.append(Message.bulkString(\"SET\"));\n    try msg_list.append(Message.bulkString(\"foo\"));\n    try msg_list.append(Message.bulkString(\"xxxyyyzzzaaabbbcccdddeeefffggghhhiiijjjkkklllmmmnnnooop\"));\n\n    const msg = Message.initList(.Array, msg_list);\n\n    const req = Request{\n        .message = msg,\n        .dict = obj_store,\n    };\n\n    const res = set(req);\n    std.debug.print(\"res: {s}\\n\", .{res.value.single});\n\n    const got = try obj_store.getString(\"foo\");\n    try expect(got != null);\n    try std.testing.expectEqualStrings(\"xxxyyyzzzaaabbbcccdddeeefffggghhhiiijjjkkklllmmmnnnooop\", got.?);\n}\n```\n\n```zig zig-redis-server/src/Server.zig\nconst std = @import(\"std\");\nconst log = std.log.scoped(.server);\nconst net = std.net;\nconst posix = std.posix;\n\nconst Server = @This();\nconst Dictionary = @import(\"Dictionary.zig\");\nconst Router = @import(\"Router.zig\");\nconst Resp = @import(\"Resp.zig\");\nconst Request = @import(\"command.zig\").Request;\nconst ThreadPool = std.Thread.Pool;\nconst xev = @import(\"xev\");\nconst TCP = xev.TCP;\n\n/// router is responsible for handling the commands received by the server.\nrouter: Router,\n\n/// allocator is used to allocate memory for the server.\nallocator: std.mem.Allocator,\n\ndict: *Dictionary,\n\n/// pool is used to handle incoming connections concurrently.\npool: *ThreadPool,\n\n/// init allocates memory for the server.\n/// Caller should call self.deinit to free the memory.\npub fn init(allocator: std.mem.Allocator) !Server {\n    const obj_store = try Dictionary.init(allocator);\n    const router = try Router.init(allocator);\n\n    const opt = ThreadPool.Options{\n        .n_jobs = 4, // TODO: Make this configurable\n        .allocator = allocator,\n    };\n    var pool = try allocator.create(ThreadPool);\n    try pool.init(opt);\n\n    return Server{\n        .allocator = allocator,\n        .router = router,\n        .dict = obj_store,\n        .pool = pool,\n    };\n}\n\npub fn deinit(self: *Server) void {\n    defer std.debug.print(\"Server deinitialised\", .{});\n\n    self.pool.deinit();\n    self.allocator.destroy(self.pool);\n    self.dict.deinit();\n    self.router.deinit();\n    self.* = undefined;\n}\n\npub fn listenAndServe(self: *Server, address: std.net.Address) !void {\n    var loop = try xev.Loop.init(.{});\n    defer loop.deinit();\n\n    // Initialize TCP server\n    const server = try TCP.init(address);\n\n    // Bind and listen\n    try server.bind(address);\n    try server.listen(128);\n\n    // Completion for accept\n    var accept_completion: xev.Completion = undefined;\n\n    const s: ?*Server = self;\n\n    // Accept loop\n    while (true) {\n        server.accept(\n            &loop,\n            &accept_completion,\n            Server,\n            s,\n            (struct {\n                fn callback(\n                    srv: ?*Server,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    result: TCP.AcceptError!TCP,\n                ) xev.CallbackAction {\n                    const connection = result catch |err| {\n                        log.err(\"Accept error, rearming accept handler: {}\", .{err});\n                        return .rearm;\n                    };\n\n                    // Handle the new connection\n                    handleConnection(srv.?, connection) catch |err| {\n                        log.err(\"Connection handling error: {}\", .{err});\n                    };\n\n                    return .rearm;\n                }\n            }).callback,\n        );\n\n        // Run the event loop\n        try loop.run(.until_done);\n    }\n}\n\nfn handleXevConnection(self: *Server, connection: TCP, loop: xev.Loop) void {\n    const resp = Resp.init(self.allocator);\n    defer resp.deinit();\n\n    const msg = resp.deserialise(connection.read()) catch |err| {\n        log.err(\"Failed to deserialise message: {}\", .{err});\n        return;\n    };\n\n    const req = Request{ .message = msg, .dict = self.dict };\n    // const resp_msg = self.router.route(req);\n\n    const raw_msg = resp.serialise(resp_msg) catch |err| {\n        log.err(\"Failed to serialise message: {}\", .{err});\n        return;\n    };\n\n    connection.write(raw_msg) catch |err| {\n        log.err(\"Failed to write to client: {}\", .{err});\n    };\n}\n\n/// handleConnection is reponsible for handling incoming connections.\n/// It reads from the client, deserialises the message, routes it to the appropriate handler,\n/// and writes the response back to the client.\n///\n/// Arguments:\n///     self: The server instance\n///     connection: The socket file descriptor for the client connection\n///\n/// Returns: void\nfn handleConnection(self: *Server, connection: posix.socket_t) void {\n    defer posix.close(connection);\n\n    var buf: [4028]u8 = undefined;\n    const read = posix.read(connection, &buf) catch |err| {\n        log.err(\"Failed to read from client: {}\", .{err});\n        return;\n    };\n\n    if (read == 0) {\n        log.info(\"Read 0 bytes from client, closing connection\", .{});\n        return;\n    }\n\n    const resp = Resp.init(self.allocator);\n    defer resp.deinit();\n\n    const msg = resp.deserialise(buf[0..read]) catch |err| {\n        log.err(\"Failed to deserialise message: {}\", .{err});\n        return;\n    };\n\n    const req = Request{ .message = msg, .dict = self.dict };\n    const resp_msg = self.router.route(req);\n\n    const raw_msg = resp.serialise(resp_msg) catch |err| {\n        log.err(\"Failed to serialise message: {}\", .{err});\n        return;\n    };\n\n    writeAll(connection, raw_msg) catch |err| {\n        log.err(\"Failed to write to client: {}\", .{err});\n    };\n}\n\nfn writeAll(socket: posix.socket_t, msg: []const u8) !void {\n    var pos: usize = 0;\n    while (pos < msg.len) {\n        const written = try posix.write(socket, msg[pos..]);\n        if (written == 0) {\n            return error.Closed;\n        }\n        pos += written;\n    }\n}\n```\n\n\nrefer tests in xev.TCP module and implement handleXevConnection function in Server.zig\n\n\nBased on the tests in xev.TCP module, I'll help you implement the `handleXevConnection` function that uses the event loop pattern. Here's the implementation:\n\n```zig\nfn handleXevConnection(self: *Server, connection: TCP, loop: *xev.Loop) void {\n    // Create buffer for reading\n    var read_buf: [4028]u8 = undefined;\n    var read_len: ?usize = null;\n    var c_read: xev.Completion = undefined;\n    \n    // Set up read completion\n    connection.read(loop, &c_read, .{ .slice = &read_buf }, ?usize, &read_len, (struct {\n    fn callback(\n        read_len_ptr: ?*?usize,\n        l: *xev.Loop,\n        c: *xev.Completion,\n        _: TCP,\n        _: xev.ReadBuffer,\n        r: TCP.ReadError!usize,\n    ) xev.CallbackAction {\n    read_len_ptr.?.* = r catch |err| {\n    log.err(\"Read error: {}\", .{err});\n    return .disarm;\n    };\n    return .disarm;\n    }\n    }).callback);\n\n    // Run the loop until read completes\n    loop.run(.until_done) catch |err| {\n        log.err(\"Loop error: {}\", .{err});\n        return;\n    };\n\n    // Check if we got any data\n    if (read_len == null or read_len.? == 0) {\n        log.info(\"No data read from client, closing connection\", .{});\n        return;\n    }\n\n    // Process the request\n    const resp = Resp.init(self.allocator);\n    defer resp.deinit();\n\n    const msg = resp.deserialise(read_buf[0..read_len.?]) catch |err| {\n        log.err(\"Failed to deserialise message: {}\", .{err});\n        return;\n    };\n\n    const req = Request{ .message = msg, .dict = self.dict };\n    const resp_msg = self.router.route(req);\n\n    const raw_msg = resp.serialise(resp_msg) catch |err| {\n        log.err(\"Failed to serialise message: {}\", .{err});\n        return;\n    };\n\n    // Write response\n    var c_write: xev.Completion = undefined;\n    var write_done = false;\n\n    connection.write(loop, &c_write, .{ .slice = raw_msg }, bool, &write_done, (struct {\n        fn callback(\n            done_ptr: ?*bool,\n            _: *xev.Loop,\n            _: *xev.Completion,\n            _: TCP,\n            _: xev.WriteBuffer,\n            r: TCP.WriteError!usize,\n        ) xev.CallbackAction {\n            _ = r catch |err| {\n                log.err(\"Write error: {}\", .{err});\n                return .disarm;\n            };\n            done_ptr.?.* = true;\n            return .disarm;\n        }\n    }).callback);\n\n    // Wait for write to complete\n    loop.run(.until_done) catch |err| {\n        log.err(\"Loop error during write: {}\", .{err});\n        return;\n    };\n\n    // Close the connection\n    var c_close: xev.Completion = undefined;\n    var close_done = false;\n\n    connection.close(loop, &c_close, bool, &close_done, (struct {\n        fn callback(\n            done_ptr: ?*bool,\n            _: *xev.Loop,\n            _: *xev.Completion,\n            _: TCP,\n            r: TCP.CloseError!void,\n        ) xev.CallbackAction {\n            _ = r catch |err| {\n                log.err(\"Close error: {}\", .{err});\n                return .disarm;\n            };\n            done_ptr.?.* = true;\n            return .disarm;\n        }\n    }).callback);\n\n    // Wait for close to complete\n    loop.run(.until_done) catch |err| {\n        log.err(\"Loop error during close: {}\", .{err});\n        return;\n    };\n}\n```\n\nKey changes made based on the xev.TCP tests:\n\n1. Used the non-blocking TCP operations with completions and callbacks\n2. Properly handled read/write operations with proper error handling\n3. Properly closed the connection when done\n4. Used the event loop pattern consistently\n\nThe function:\n1. Sets up a read buffer and reads from the connection\n2. Processes the request (deserialize, route, serialize response)\n3. Writes the response back to the client\n4. Properly closes the connection\n\nEach operation (read/write/close) follows the pattern from the tests:\n1. Creates a completion\n2. Sets up a callback\n3. Waits for the operation to complete using the event loop\n\nThe error handling is also improved, with proper logging of errors at each step.\n\nYou'll need to modify your `listenAndServe` function to pass the loop pointer to `handleXevConnection`:\n\n```zig\n// In your accept callback:\nfn callback(\n    srv: ?*Server,\n    l: *xev.Loop,  // Add loop parameter\n    _: *xev.Completion,\n    result: TCP.AcceptError!TCP,\n) xev.CallbackAction {\n    const connection = result catch |err| {\n        log.err(\"Accept error, rearming accept handler: {}\", .{err});\n        return .rearm;\n    };\n\n    // Pass the loop to handleXevConnection\n    handleXevConnection(srv.?, connection, l) catch |err| {\n        log.err(\"Connection handling error: {}\", .{err});\n    };\n\n    return .rearm;\n}\n```\n\nThis implementation should provide non-blocking I/O handling while maintaining proper connection lifecycle management.\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":287}}},{"id":{"replica_id":0,"value":94},"start":62985,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":288}}},{"id":{"replica_id":0,"value":95},"start":67715,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":289}}}],"summary":"Implement TCP Connection Handler XEV","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null},{"range":{"start":3,"end":23614},"icon":"File","label":"/home/msharran/.cache/zig/p/12205b2b47fe61a4cde3a45ee4b9cddee75897739dbc196d6396e117cb1ce28e1ad0/src/watcher/tcp.zig","metadata":{"path":"/home/msharran/.cache/zig/p/12205b2b47fe61a4cde3a45ee4b9cddee75897739dbc196d6396e117cb1ce28e1ad0/src/watcher/tcp.zig"}},{"range":{"start":23615,"end":53996},"icon":"File","label":"/home/msharran/.cache/zig/p/12205b2b47fe61a4cde3a45ee4b9cddee75897739dbc196d6396e117cb1ce28e1ad0/src/watcher/stream.zig","metadata":{"path":"/home/msharran/.cache/zig/p/12205b2b47fe61a4cde3a45ee4b9cddee75897739dbc196d6396e117cb1ce28e1ad0/src/watcher/stream.zig"}},{"range":{"start":53997,"end":57680},"icon":"File","label":"zig-redis-server/src/command.zig","metadata":{"path":"zig-redis-server/src/command.zig"}},{"range":{"start":57681,"end":62894},"icon":"File","label":"zig-redis-server/src/Server.zig","metadata":{"path":"zig-redis-server/src/Server.zig"}}]}