{"id":"c29499de-5fa7-4f9b-962a-64438185f8b8","zed":"context","version":"0.4.0","text":"\n\n\n```zig /home/msharran/.cache/zig/p/12205b2b47fe61a4cde3a45ee4b9cddee75897739dbc196d6396e117cb1ce28e1ad0/src/watcher/tcp.zig\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst assert = std.debug.assert;\nconst posix = std.posix;\nconst stream = @import(\"stream.zig\");\nconst common = @import(\"common.zig\");\n\n/// TCP client and server.\n///\n/// This is a \"higher-level abstraction\" in libxev. The goal of higher-level\n/// abstractions in libxev are to make it easier to use specific functionality\n/// with the event loop, but does not promise perfect flexibility or optimal\n/// performance. In almost all cases, the abstraction is good enough. But,\n/// if you have specific needs or want to push for the most optimal performance,\n/// use the platform-specific Loop directly.\npub fn TCP(comptime xev: type) type {\n    return struct {\n        const Self = @This();\n        const FdType = if (xev.backend == .iocp) std.os.windows.HANDLE else posix.socket_t;\n\n        fd: FdType,\n\n        pub usingnamespace stream.Stream(xev, Self, .{\n            .close = true,\n            .read = .recv,\n            .write = .send,\n        });\n\n        /// Initialize a new TCP with the family from the given address. Only\n        /// the family is used, the actual address has no impact on the created\n        /// resource.\n        pub fn init(addr: std.net.Address) !Self {\n            if (xev.backend == .wasi_poll) @compileError(\"unsupported in WASI\");\n\n            const fd = if (xev.backend == .iocp)\n                try std.os.windows.WSASocketW(addr.any.family, posix.SOCK.STREAM, 0, null, 0, std.os.windows.ws2_32.WSA_FLAG_OVERLAPPED)\n            else fd: {\n                // On io_uring we don't use non-blocking sockets because we may\n                // just get EAGAIN over and over from completions.\n                const flags = flags: {\n                    var flags: u32 = posix.SOCK.STREAM | posix.SOCK.CLOEXEC;\n                    if (xev.backend != .io_uring) flags |= posix.SOCK.NONBLOCK;\n                    break :flags flags;\n                };\n                break :fd try posix.socket(addr.any.family, flags, 0);\n            };\n\n            return .{\n                .fd = fd,\n            };\n        }\n\n        /// Initialize a TCP socket from a file descriptor.\n        pub fn initFd(fd: FdType) Self {\n            return .{\n                .fd = fd,\n            };\n        }\n\n        /// Bind the address to the socket.\n        pub fn bind(self: Self, addr: std.net.Address) !void {\n            if (xev.backend == .wasi_poll) @compileError(\"unsupported in WASI\");\n\n            const fd = if (xev.backend == .iocp) @as(std.os.windows.ws2_32.SOCKET, @ptrCast(self.fd)) else self.fd;\n\n            try posix.setsockopt(fd, posix.SOL.SOCKET, posix.SO.REUSEADDR, &std.mem.toBytes(@as(c_int, 1)));\n            try posix.bind(fd, &addr.any, addr.getOsSockLen());\n        }\n\n        /// Listen for connections on the socket. This puts the socket into passive\n        /// listening mode. Connections must still be accepted one at a time.\n        pub fn listen(self: Self, backlog: u31) !void {\n            if (xev.backend == .wasi_poll) @compileError(\"unsupported in WASI\");\n\n            const fd = if (xev.backend == .iocp) @as(std.os.windows.ws2_32.SOCKET, @ptrCast(self.fd)) else self.fd;\n\n            try posix.listen(fd, backlog);\n        }\n\n        /// Accept a single connection.\n        pub fn accept(\n            self: Self,\n            loop: *xev.Loop,\n            c: *xev.Completion,\n            comptime Userdata: type,\n            userdata: ?*Userdata,\n            comptime cb: *const fn (\n                ud: ?*Userdata,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: AcceptError!Self,\n            ) xev.CallbackAction,\n        ) void {\n            c.* = .{\n                .op = .{\n                    .accept = .{\n                        .socket = self.fd,\n                    },\n                },\n\n                .userdata = userdata,\n                .callback = (struct {\n                    fn callback(\n                        ud: ?*anyopaque,\n                        l_inner: *xev.Loop,\n                        c_inner: *xev.Completion,\n                        r: xev.Result,\n                    ) xev.CallbackAction {\n                        return @call(.always_inline, cb, .{\n                            common.userdataValue(Userdata, ud),\n                            l_inner,\n                            c_inner,\n                            if (r.accept) |fd| initFd(fd) else |err| err,\n                        });\n                    }\n                }).callback,\n            };\n\n            // If we're dup-ing, then we ask the backend to manage the fd.\n            switch (xev.backend) {\n                .io_uring,\n                .kqueue,\n                .wasi_poll,\n                .iocp,\n                => {},\n\n                .epoll => c.flags.dup = true,\n            }\n\n            loop.add(c);\n        }\n\n        /// Establish a connection as a client.\n        pub fn connect(\n            self: Self,\n            loop: *xev.Loop,\n            c: *xev.Completion,\n            addr: std.net.Address,\n            comptime Userdata: type,\n            userdata: ?*Userdata,\n            comptime cb: *const fn (\n                ud: ?*Userdata,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                s: Self,\n                r: ConnectError!void,\n            ) xev.CallbackAction,\n        ) void {\n            if (xev.backend == .wasi_poll) @compileError(\"unsupported in WASI\");\n\n            c.* = .{\n                .op = .{\n                    .connect = .{\n                        .socket = self.fd,\n                        .addr = addr,\n                    },\n                },\n\n                .userdata = userdata,\n                .callback = (struct {\n                    fn callback(\n                        ud: ?*anyopaque,\n                        l_inner: *xev.Loop,\n                        c_inner: *xev.Completion,\n                        r: xev.Result,\n                    ) xev.CallbackAction {\n                        return @call(.always_inline, cb, .{\n                            common.userdataValue(Userdata, ud),\n                            l_inner,\n                            c_inner,\n                            initFd(c_inner.op.connect.socket),\n                            if (r.connect) |_| {} else |err| err,\n                        });\n                    }\n                }).callback,\n            };\n\n            loop.add(c);\n        }\n\n        /// Shutdown the socket. This always only shuts down the writer side. You\n        /// can use the lower level interface directly to control this if the\n        /// platform supports it.\n        pub fn shutdown(\n            self: Self,\n            loop: *xev.Loop,\n            c: *xev.Completion,\n            comptime Userdata: type,\n            userdata: ?*Userdata,\n            comptime cb: *const fn (\n                ud: ?*Userdata,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                s: Self,\n                r: ShutdownError!void,\n            ) xev.CallbackAction,\n        ) void {\n            c.* = .{\n                .op = .{\n                    .shutdown = .{\n                        .socket = self.fd,\n                        .how = .send,\n                    },\n                },\n                .userdata = userdata,\n                .callback = (struct {\n                    fn callback(\n                        ud: ?*anyopaque,\n                        l_inner: *xev.Loop,\n                        c_inner: *xev.Completion,\n                        r: xev.Result,\n                    ) xev.CallbackAction {\n                        return @call(.always_inline, cb, .{\n                            common.userdataValue(Userdata, ud),\n                            l_inner,\n                            c_inner,\n                            initFd(c_inner.op.shutdown.socket),\n                            if (r.shutdown) |_| {} else |err| err,\n                        });\n                    }\n                }).callback,\n            };\n\n            loop.add(c);\n        }\n\n        pub const AcceptError = xev.AcceptError;\n        pub const ConnectError = xev.ConnectError;\n        pub const ShutdownError = xev.ShutdownError;\n\n        test \"TCP: accept/connect/send/recv/close\" {\n            // We have no way to get a socket in WASI from a WASI context.\n            if (xev.backend == .wasi_poll) return error.SkipZigTest;\n\n            const testing = std.testing;\n\n            var loop = try xev.Loop.init(.{});\n            defer loop.deinit();\n\n            // Choose random available port (Zig #14907)\n            var address = try std.net.Address.parseIp4(\"127.0.0.1\", 0);\n            const server = try Self.init(address);\n\n            // Bind and listen\n            try server.bind(address);\n            try server.listen(1);\n\n            // Retrieve bound port and initialize client\n            var sock_len = address.getOsSockLen();\n            const fd = if (xev.backend == .iocp) @as(std.os.windows.ws2_32.SOCKET, @ptrCast(server.fd)) else server.fd;\n            try posix.getsockname(fd, &address.any, &sock_len);\n            const client = try Self.init(address);\n\n            //const address = try std.net.Address.parseIp4(\"127.0.0.1\", 3132);\n            //var server = try Self.init(address);\n            //var client = try Self.init(address);\n\n            // Completions we need\n            var c_accept: xev.Completion = undefined;\n            var c_connect: xev.Completion = undefined;\n\n            // Accept\n            var server_conn: ?Self = null;\n            server.accept(&loop, &c_accept, ?Self, &server_conn, (struct {\n                fn callback(\n                    ud: ?*?Self,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    r: AcceptError!Self,\n                ) xev.CallbackAction {\n                    ud.?.* = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Connect\n            var connected: bool = false;\n            client.connect(&loop, &c_connect, address, bool, &connected, (struct {\n                fn callback(\n                    ud: ?*bool,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: ConnectError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = true;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Wait for the connection to be established\n            try loop.run(.until_done);\n            try testing.expect(server_conn != null);\n            try testing.expect(connected);\n\n            // Close the server\n            var server_closed = false;\n            server.close(&loop, &c_accept, bool, &server_closed, (struct {\n                fn callback(\n                    ud: ?*bool,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: Self.CloseError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = true;\n                    return .disarm;\n                }\n            }).callback);\n            try loop.run(.until_done);\n            try testing.expect(server_closed);\n\n            // Send\n            var send_buf = [_]u8{ 1, 1, 2, 3, 5, 8, 13 };\n            client.write(&loop, &c_connect, .{ .slice = &send_buf }, void, null, (struct {\n                fn callback(\n                    _: ?*void,\n                    _: *xev.Loop,\n                    c: *xev.Completion,\n                    _: Self,\n                    _: xev.WriteBuffer,\n                    r: Self.WriteError!usize,\n                ) xev.CallbackAction {\n                    _ = c;\n                    _ = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Receive\n            var recv_buf: [128]u8 = undefined;\n            var recv_len: usize = 0;\n            server_conn.?.read(&loop, &c_accept, .{ .slice = &recv_buf }, usize, &recv_len, (struct {\n                fn callback(\n                    ud: ?*usize,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    _: xev.ReadBuffer,\n                    r: Self.ReadError!usize,\n                ) xev.CallbackAction {\n                    ud.?.* = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Wait for the send/receive\n            try loop.run(.until_done);\n            try testing.expectEqualSlices(u8, &send_buf, recv_buf[0..recv_len]);\n\n            // Close\n            server_conn.?.close(&loop, &c_accept, ?Self, &server_conn, (struct {\n                fn callback(\n                    ud: ?*?Self,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: Self.CloseError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = null;\n                    return .disarm;\n                }\n            }).callback);\n            client.close(&loop, &c_connect, bool, &connected, (struct {\n                fn callback(\n                    ud: ?*bool,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: Self.CloseError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = false;\n                    return .disarm;\n                }\n            }).callback);\n\n            try loop.run(.until_done);\n            try testing.expect(server_conn == null);\n            try testing.expect(!connected);\n            try testing.expect(server_closed);\n        }\n\n        // Potentially flaky - this test could hang if the sender is unable to\n        // write everything to the socket for whatever reason\n        // (e.g. incorrectly sized buffer on the receiver side), or if the\n        // receiver is trying to receive while sender has nothing left to send.\n        //\n        // Overview:\n        // 1. Set up server and client sockets\n        // 2. connect & accept, set SO_SNDBUF to 8kB on the client\n        // 3. Try to send 1MB buffer from client to server without queuing, this _should_ fail\n        //    and theoretically send <= 8kB, but in practice, it seems to write ~32kB.\n        //    Asserts that <= 100kB was written\n        // 4. Set up a queued write with the remaining buffer, shutdown() the socket afterwards\n        // 5. Set up a receiver that loops until it receives the entire buffer\n        // 6. Assert send_buf == recv_buf\n        test \"TCP: Queued writes\" {\n            // We have no way to get a socket in WASI from a WASI context.\n            if (xev.backend == .wasi_poll) return error.SkipZigTest;\n            // Windows doesn't seem to respect the SNDBUF socket option.\n            if (builtin.os.tag == .windows) return error.SkipZigTest;\n\n            const testing = std.testing;\n\n            var loop = try xev.Loop.init(.{});\n            defer loop.deinit();\n\n            // Choose random available port (Zig #14907)\n            var address = try std.net.Address.parseIp4(\"127.0.0.1\", 0);\n            const server = try Self.init(address);\n\n            // Bind and listen\n            try server.bind(address);\n            try server.listen(1);\n\n            // Retrieve bound port and initialize client\n            var sock_len = address.getOsSockLen();\n            try posix.getsockname(server.fd, &address.any, &sock_len);\n            const client = try Self.init(address);\n\n            // Completions we need\n            var c_accept: xev.Completion = undefined;\n            var c_connect: xev.Completion = undefined;\n\n            // Accept\n            var server_conn: ?Self = null;\n            server.accept(&loop, &c_accept, ?Self, &server_conn, (struct {\n                fn callback(\n                    ud: ?*?Self,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    r: AcceptError!Self,\n                ) xev.CallbackAction {\n                    ud.?.* = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Connect\n            var connected: bool = false;\n            client.connect(&loop, &c_connect, address, bool, &connected, (struct {\n                fn callback(\n                    ud: ?*bool,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: ConnectError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = true;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Wait for the connection to be established\n            try loop.run(.until_done);\n            try testing.expect(server_conn != null);\n            try testing.expect(connected);\n\n            // Close the server\n            var server_closed = false;\n            server.close(&loop, &c_accept, bool, &server_closed, (struct {\n                fn callback(\n                    ud: ?*bool,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: Self.CloseError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = true;\n                    return .disarm;\n                }\n            }).callback);\n            try loop.run(.until_done);\n            try testing.expect(server_closed);\n\n            // Unqueued send - Limit send buffer to 8kB, this should force partial writes.\n            try posix.setsockopt(client.fd, posix.SOL.SOCKET, posix.SO.SNDBUF, &std.mem.toBytes(@as(c_int, 8192)));\n\n            const send_buf = [_]u8{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 0 } ** 100_000;\n            var sent_unqueued: usize = 0;\n\n            // First we try to send the whole 1MB buffer in one write operation, this _should_ result\n            // in a partial write.\n            client.write(&loop, &c_connect, .{ .slice = &send_buf }, usize, &sent_unqueued, (struct {\n                fn callback(\n                    sent_unqueued_inner: ?*usize,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    _: xev.WriteBuffer,\n                    r: Self.WriteError!usize,\n                ) xev.CallbackAction {\n                    sent_unqueued_inner.?.* = r catch unreachable;\n                    return .disarm;\n                }\n            }).callback);\n\n            // Make sure that we sent a small fraction of the buffer\n            try loop.run(.until_done);\n            // SO_SNDBUF doesn't seem to be respected exactly, sent_unqueued will often be ~32kB\n            // even though SO_SNDBUF was set to 8kB\n            try testing.expect(sent_unqueued < (send_buf.len / 10));\n\n            // Set up queued write\n            var w_queue = Self.WriteQueue{};\n            var wr_send: xev.TCP.WriteRequest = undefined;\n            var sent_queued: usize = 0;\n            const queued_slice = send_buf[sent_unqueued..];\n            client.queueWrite(&loop, &w_queue, &wr_send, .{ .slice = queued_slice }, usize, &sent_queued, (struct {\n                fn callback(\n                    sent_queued_inner: ?*usize,\n                    l: *xev.Loop,\n                    c: *xev.Completion,\n                    tcp: Self,\n                    _: xev.WriteBuffer,\n                    r: Self.WriteError!usize,\n                ) xev.CallbackAction {\n                    sent_queued_inner.?.* = r catch unreachable;\n\n                    tcp.shutdown(l, c, void, null, (struct {\n                        fn callback(\n                            _: ?*void,\n                            _: *xev.Loop,\n                            _: *xev.Completion,\n                            _: Self,\n                            _: Self.ShutdownError!void,\n                        ) xev.CallbackAction {\n                            return .disarm;\n                        }\n                    }).callback);\n\n                    return .disarm;\n                }\n            }).callback);\n\n            // Set up receiver which is going to keep reading until it reads the full\n            // send buffer\n            const Receiver = struct {\n                loop: *xev.Loop,\n                conn: Self,\n                completion: xev.Completion = .{},\n                buf: [send_buf.len]u8 = undefined,\n                bytes_read: usize = 0,\n\n                pub fn read(receiver: *@This()) void {\n                    if (receiver.bytes_read == receiver.buf.len) return;\n\n                    const read_buf = xev.ReadBuffer{\n                        .slice = receiver.buf[receiver.bytes_read..],\n                    };\n                    receiver.conn.read(receiver.loop, &receiver.completion, read_buf, @This(), receiver, readCb);\n                }\n\n                pub fn readCb(\n                    receiver_opt: ?*@This(),\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    _: xev.ReadBuffer,\n                    r: Self.ReadError!usize,\n                ) xev.CallbackAction {\n                    var receiver = receiver_opt.?;\n                    const n_bytes = r catch unreachable;\n\n                    receiver.bytes_read += n_bytes;\n                    if (receiver.bytes_read < send_buf.len) {\n                        receiver.read();\n                    }\n\n                    return .disarm;\n                }\n            };\n            var receiver = Receiver{\n                .loop = &loop,\n                .conn = server_conn.?,\n            };\n            receiver.read();\n\n            // Wait for the send/receive\n            try loop.run(.until_done);\n            try testing.expectEqualSlices(u8, &send_buf, receiver.buf[0..receiver.bytes_read]);\n            try testing.expect(send_buf.len == sent_unqueued + sent_queued);\n\n            // Close\n            server_conn.?.close(&loop, &c_accept, ?Self, &server_conn, (struct {\n                fn callback(\n                    ud: ?*?Self,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: Self.CloseError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = null;\n                    return .disarm;\n                }\n            }).callback);\n            client.close(&loop, &c_connect, bool, &connected, (struct {\n                fn callback(\n                    ud: ?*bool,\n                    _: *xev.Loop,\n                    _: *xev.Completion,\n                    _: Self,\n                    r: Self.CloseError!void,\n                ) xev.CallbackAction {\n                    _ = r catch unreachable;\n                    ud.?.* = false;\n                    return .disarm;\n                }\n            }).callback);\n\n            try loop.run(.until_done);\n            try testing.expect(server_conn == null);\n            try testing.expect(!connected);\n            try testing.expect(server_closed);\n        }\n    };\n}\n```\n\n```zig zig-redis-server/src/TcpServer.zig\nconst std = @import(\"std\");\nconst std_log = std.log;\nconst log = std_log.scoped(.server);\nconst net = std.net;\nconst posix = std.posix;\n\nconst TcpServer = @This();\nconst Dictionary = @import(\"Dictionary.zig\");\nconst Router = @import(\"Router.zig\");\nconst Resp = @import(\"Resp.zig\");\nconst Request = @import(\"command.zig\").Request;\nconst ThreadPool = std.Thread.Pool;\nconst xev = @import(\"xev\");\nconst TCP = xev.TCP;\nconst uuid = @import(\"uuid\");\n\n/// router is responsible for handling the commands received by the server.\nrouter: Router,\n\n/// allocator is used to allocate memory for the server.\nallocator: std.mem.Allocator,\n\ndict: *Dictionary,\n\n/// pool is used to handle incoming connections concurrently.\npool: *ThreadPool,\n\n/// init allocates memory for the server.\n/// Caller should call self.deinit to free the memory.\npub fn init(allocator: std.mem.Allocator) !TcpServer {\n    const obj_store = try Dictionary.init(allocator);\n    const router = try Router.init(allocator);\n\n    const opt = ThreadPool.Options{\n        .n_jobs = 4, // TODO: Make this configurable\n        .allocator = allocator,\n    };\n    var pool = try allocator.create(ThreadPool);\n    try pool.init(opt);\n\n    return TcpServer{\n        .allocator = allocator,\n        .router = router,\n        .dict = obj_store,\n        .pool = pool,\n    };\n}\n\npub fn deinit(self: *TcpServer) void {\n    defer std.debug.print(\"Server deinitialised\", .{});\n\n    self.pool.deinit();\n    self.allocator.destroy(self.pool);\n    self.dict.deinit();\n    self.router.deinit();\n    self.* = undefined;\n}\n\npub fn listenAndServe(self: *TcpServer, address: std.net.Address) !void {\n    var loop = try xev.Loop.init(.{});\n    defer loop.deinit();\n\n    // Initialize TCP server\n    var tcp = try TCP.init(address);\n\n    // Bind and listen\n    try tcp.bind(address);\n    try tcp.listen(128);\n\n    // Accept loop\n    // while (true) {\n    const id = uuid.v7.new();\n    const urn = uuid.urn.serialize(id);\n    var ctx = try Context.init(self.allocator, tcp, loop, urn);\n\n    log.info(\"Waiting for incoming connections\", .{});\n    tcp.accept(&loop, ctx.accept_completion, Context, &ctx, acceptCallback);\n    try loop.run(.until_done); // FIXME: change to .until_done after debugging\n    // }\n}\n\n/// handleConnection is reponsible for handling incoming connections.\n/// It reads from the client, deserialises the message, routes it to the appropriate handler,\n/// and writes the response back to the client.\n///\n/// Arguments:\n///     self: The server instance\n///     connection: The socket file descriptor for the client connection\n///\n/// Returns: void\nfn handleConnection(self: *TcpServer, connection: posix.socket_t) void {\n    defer posix.close(connection);\n\n    var buf: [4028]u8 = undefined;\n    const read = posix.read(connection, &buf) catch |err| {\n        log.err(\"Failed to read from client: {}\", .{err});\n        return;\n    };\n\n    if (read == 0) {\n        log.info(\"Read 0 bytes from client, closing connection\", .{});\n        return;\n    }\n\n    const resp = Resp.init(self.allocator);\n    defer resp.deinit();\n\n    const msg = resp.deserialise(buf[0..read]) catch |err| {\n        log.err(\"Failed to deserialise message: {}\", .{err});\n        return;\n    };\n\n    const req = Request{ .message = msg, .dict = self.dict };\n    const resp_msg = self.router.route(req);\n\n    const raw_msg = resp.serialise(resp_msg) catch |err| {\n        log.err(\"Failed to serialise message: {}\", .{err});\n        return;\n    };\n\n    writeAll(connection, raw_msg) catch |err| {\n        log.err(\"Failed to write to client: {}\", .{err});\n    };\n}\n\nfn writeAll(socket: posix.socket_t, msg: []const u8) !void {\n    var pos: usize = 0;\n    while (pos < msg.len) {\n        const written = try posix.write(socket, msg[pos..]);\n        if (written == 0) {\n            return error.Closed;\n        }\n        pos += written;\n    }\n}\n\n// callbacks from event loop\n\nconst Context = struct {\n    tcp: TCP,\n    loop: xev.Loop,\n    connection_id: [36]u8,\n    allocator: std.mem.Allocator,\n\n    // allocate them in heap\n    accept_completion: *xev.Completion,\n    close_completion: *xev.Completion,\n    read_completion: *xev.Completion,\n    read_buf: []u8,\n    write_completion: *xev.Completion,\n    write_buf: []u8,\n\n    pub fn init(allocator: std.mem.Allocator, tcp: TCP, loop: xev.Loop, urn: [36]u8) !Context {\n        log.debug(\"context initialized for connection={s}\", .{urn});\n        return Context{\n            .tcp = tcp,\n            .loop = loop,\n            .connection_id = urn,\n            .allocator = allocator,\n            .accept_completion = try allocator.create(xev.Completion),\n            .close_completion = try allocator.create(xev.Completion),\n            .read_completion = try allocator.create(xev.Completion),\n            .write_completion = try allocator.create(xev.Completion),\n            .read_buf = try allocator.alloc(u8, 4096),\n            .write_buf = try allocator.alloc(u8, 4096),\n        };\n    }\n\n    pub fn deinit(self: *Context) void {\n        self.allocator.destroy(self.accept_completion);\n        self.allocator.destroy(self.close_completion);\n        self.allocator.destroy(self.read_completion);\n        self.allocator.destroy(self.write_completion);\n        self.allocator.free(self.read_buf);\n        self.allocator.free(self.write_buf);\n        log.debug(\"context deinitialized for connection={s}\", .{self.connection_id});\n        self.* = undefined;\n    }\n};\n\nfn acceptCallback(\n    ctx: ?*Context,\n    loop: *xev.Loop,\n    _: *xev.Completion,\n    result: xev.TCP.AcceptError!xev.TCP,\n) xev.CallbackAction {\n    const l = std_log.scoped(.event_accept);\n    l.debug(\"connection_id={?s}\", .{ctx.?.*.connection_id});\n\n    // Handle accept errors\n    var tcp = result catch |err| {\n        l.err(\"result err: {}\", .{err});\n        return .disarm;\n    };\n\n    tcp.read(\n        loop,\n        ctx.?.read_completion,\n        .{ .slice = ctx.?.*.read_buf },\n        Context,\n        ctx,\n        readCallback,\n    );\n\n    return .disarm;\n}\n\nfn readCallback(\n    ctx: ?*Context,\n    loop: *xev.Loop,\n    _: *xev.Completion,\n    _: xev.TCP,\n    read_buf: xev.ReadBuffer,\n    result: xev.TCP.ReadError!usize,\n) xev.CallbackAction {\n    const l = std_log.scoped(.event_read);\n    l.debug(\"connection_id={?s}\", .{ctx.?.*.connection_id});\n    var tcp = ctx.?.tcp;\n\n    // Handle read errors\n    const bytes_read = result catch |err| {\n        l.err(\"read error, closing connection: {}\", .{err});\n        tcp.close(loop, ctx.?.close_completion, Context, ctx, closeCallback);\n        return .disarm;\n    };\n\n    // Check for EOF\n    if (bytes_read == 0) {\n        l.err(\"EOF during read, closing connection\", .{});\n        tcp.close(loop, ctx.?.close_completion, Context, ctx, closeCallback);\n        return .disarm;\n    }\n\n    // Echo the data back\n    // FIXME: check and fix this\n    const write_buf = read_buf.slice[0..bytes_read];\n    l.debug(\"echoing back content={s}\", .{write_buf});\n\n    tcp.write(\n        loop,\n        ctx.?.write_completion,\n        .{ .slice = write_buf },\n        Context,\n        ctx,\n        writeCallback,\n    );\n\n    // will be rearmed in acceptCallback\n    return .disarm;\n}\n\nfn writeCallback(\n    ctx: ?*Context,\n    loop: *xev.Loop,\n    _: *xev.Completion,\n    tcp: xev.TCP,\n    write_buf: xev.WriteBuffer,\n    result: xev.TCP.WriteError!usize,\n) xev.CallbackAction {\n    const l = std_log.scoped(.event_write);\n    l.debug(\"connection_id={?s} \", .{ctx.?.*.connection_id});\n\n    // Handle write errors\n    const written = result catch |err| {\n        l.err(\"Write error, closing conn: {}\", .{err});\n        tcp.close(loop, ctx.?.close_completion, Context, ctx, closeCallback);\n        return .disarm;\n    };\n\n    l.debug(\"Wrote data={s}, len={d}, closing connection\", .{ write_buf.slice, written });\n\n    // Since we have successfully written to the connection, we can close it\n    tcp.close(loop, ctx.?.close_completion, Context, ctx, closeCallback);\n\n    return .disarm;\n}\n\nfn closeCallback(\n    ctx: ?*Context,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    _: xev.TCP,\n    result: xev.TCP.CloseError!void,\n) xev.CallbackAction {\n    const l = std_log.scoped(.event_close);\n    l.debug(\"connection_id={?s}\", .{ctx.?.*.connection_id});\n\n    if (result) {} else |err| {\n        l.err(\"Close error: {}\", .{err});\n    }\n\n    l.debug(\"connection closed, deallocating memory\", .{});\n    ctx.?.deinit();\n    return .disarm;\n}\n```\n\n```zig /home/msharran/.cache/zig/p/12205b2b47fe61a4cde3a45ee4b9cddee75897739dbc196d6396e117cb1ce28e1ad0/src/backend/iocp.zig\n//! Backend to use win32 IOCP.\nconst std = @import(\"std\");\nconst assert = std.debug.assert;\nconst windows = @import(\"../windows.zig\");\nconst queue = @import(\"../queue.zig\");\nconst heap = @import(\"../heap.zig\");\nconst xev = @import(\"../main.zig\").IOCP;\nconst posix = std.posix;\n\nconst log = std.log.scoped(.libxev_iocp);\n\npub const Loop = struct {\n    const TimerHeap = heap.Intrusive(Timer, void, Timer.less);\n\n    /// The handle to the IO completion port.\n    iocp_handle: windows.HANDLE = windows.INVALID_HANDLE_VALUE,\n\n    /// The number of active completions. This DOES NOT include completions that are queued in the\n    /// submissions queue.\n    active: usize = 0,\n\n    /// Our queue of submissions that we want to enqueue on the next tick.\n    /// These are NOT started.\n    submissions: queue.Intrusive(Completion) = .{},\n\n    /// The queue of cancellation requests. These will point to the completion that we need to\n    /// cancel. We don't enqueue the exact completion to cancel because it may be in another queue.\n    cancellations: queue.Intrusive(Completion) = .{},\n\n    /// Our queue of completed completions where the callback hasn't been called yet, but the\n    /// \"result\" field should be set on every completion. This is used to delay completion callbacks\n    /// until the next tick.\n    completions: queue.Intrusive(Completion) = .{},\n\n    /// Our queue of waiting completions\n    asyncs: queue.Intrusive(Completion) = .{},\n\n    /// Heap of timers.\n    timers: TimerHeap = .{ .context = {} },\n\n    /// Cached time\n    cached_now: u64,\n\n    /// Duration of a tick of Windows QueryPerformanceCounter.\n    qpc_duration: u64,\n\n    /// Some internal fields we can pack for better space.\n    flags: packed struct {\n        /// Whether we're in a run of not (to prevent nested runs).\n        in_run: bool = false,\n\n        /// Whether our loop is in a stopped state or not.\n        stopped: bool = false,\n    } = .{},\n\n    /// Initialize a new IOCP-backed event loop. See the Options docs\n    /// for what options matter for IOCP.\n    pub fn init(options: xev.Options) !Loop {\n        _ = options;\n\n        // Get the duration of the QueryPerformanceCounter.\n        // We should check if the division is lossless, but it returns 10_000_000 on my machine so\n        // we'll handle that later.\n        const qpc_duration = 1_000_000_000 / windows.QueryPerformanceFrequency();\n\n        // This creates a new Completion Port\n        const handle = try windows.CreateIoCompletionPort(windows.INVALID_HANDLE_VALUE, null, 0, 1);\n        var res: Loop = .{\n            .iocp_handle = handle,\n            .qpc_duration = qpc_duration,\n            .cached_now = undefined,\n        };\n\n        res.update_now();\n        return res;\n    }\n\n    /// Deinitialize the loop, this closes the handle to the Completion Port. Any events that were\n    /// unprocessed are lost -- their callbacks will never be called.\n    pub fn deinit(self: *Loop) void {\n        windows.CloseHandle(self.iocp_handle);\n    }\n\n    /// Stop the loop. This can only be called from the main thread.\n    /// This will stop the loop forever. Future ticks will do nothing.\n    ///\n    /// This does NOT stop any completions associated to operations that are in-flight.\n    pub fn stop(self: *Loop) void {\n        self.flags.stopped = true;\n    }\n\n    /// Add a completion to the loop. The completion is not started until the loop is run (`run`) or\n    /// an explicit submission request is made (`submit`).\n    pub fn add(self: *Loop, completion: *Completion) void {\n        // If the completion is a cancel operation, we start it immediately as it will be put in the\n        // cancellations queue.\n        if (completion.op == .cancel) {\n            self.start_completion(completion);\n            return;\n        }\n\n        switch (completion.flags.state) {\n            // The completion is in an adding state already, nothing needs to be done.\n            .adding => return,\n\n            // The completion is dead, probably because it was canceled.\n            .dead => {},\n\n            // If we reach this point, we have a problem...\n            .active => unreachable,\n        }\n\n        // We just add the completion to the queue. Failures can happen\n        // at submission or tick time.\n        completion.flags.state = .adding;\n        self.submissions.push(completion);\n    }\n\n    /// Submit any enqueued completions. This does not fire any callbacks for completed events\n    /// (success or error). Callbacks are only fired on the next tick.\n    pub fn submit(self: *Loop) !void {\n        // Submit all the submissions. We copy the submission queue so that any resubmits don't\n        // cause an infinite loop.\n        var queued = self.submissions;\n        self.submissions = .{};\n\n        // On error, we have to restore the queue because we may be batching.\n        errdefer self.submissions = queued;\n\n        while (queued.pop()) |c| {\n            switch (c.flags.state) {\n                .adding => self.start_completion(c),\n                .dead => self.stop_completion(c, null),\n                .active => std.log.err(\n                    \"invalid state in submission queue state={}\",\n                    .{c.flags.state},\n                ),\n            }\n        }\n    }\n\n    /// Process the cancellations queue. This doesn't call any callbacks but can potentially make\n    /// system call to cancel active IO.\n    fn process_cancellations(self: *Loop) void {\n        while (self.cancellations.pop()) |c| {\n            const target = c.op.cancel.c;\n            var cancel_result: CancelError!void = {};\n            switch (target.flags.state) {\n                // If the target is dead already we do nothing.\n                .dead => {},\n\n                // If it is in the submission queue, mark them as dead so they will never be\n                // submitted.\n                .adding => target.flags.state = .dead,\n\n                // If it is active we need to schedule the deletion.\n                .active => self.stop_completion(target, &cancel_result),\n            }\n\n            // We completed the cancellation.\n            c.result = .{ .cancel = cancel_result };\n            self.completions.push(c);\n        }\n    }\n\n    /// Run the event loop. See RunMode documentation for details on modes.\n    /// Once the loop is run, the pointer MUST remain stable.\n    pub fn run(self: *Loop, mode: xev.RunMode) !void {\n        switch (mode) {\n            .no_wait => try self.tick(0),\n            .once => try self.tick(1),\n            .until_done => while (!self.done()) try self.tick(1),\n        }\n    }\n\n    /// Tick through the event loop once, waiting for at least \"wait\" completions to be processed by\n    /// the loop itself.\n    pub fn tick(self: *Loop, wait: u32) !void {\n        // If we're stopped then the loop is fully over.\n        if (self.flags.stopped) return;\n\n        // We can't nest runs.\n        if (self.flags.in_run) return error.NestedRunsNotAllowed;\n        self.flags.in_run = true;\n        defer self.flags.in_run = false;\n\n        // The list of entry that will be filled with a call to GetQueuedCompletionStatusEx.\n        var entries: [128]windows.OVERLAPPED_ENTRY = undefined;\n\n        var wait_rem = @as(usize, @intCast(wait));\n\n        // Handle all of our cancellations first because we may be able to stop submissions from\n        // even happening if its still queued. Plus, cancellations sometimes add more to the\n        // submission queue.\n        self.process_cancellations();\n\n        // Submit pending completions.\n        try self.submit();\n\n        // Loop condition is inspired from the kqueue backend. See its documentation for details.\n        while (true) {\n            // If we're stopped then the loop is fully over.\n            if (self.flags.stopped) return;\n\n            // We must update our time no matter what.\n            self.update_now();\n\n            const should_continue = (self.active > 0 and (wait == 0 or wait_rem > 0)) or !self.completions.empty();\n            if (!should_continue) break;\n\n            // Run our expired timers.\n            const now_timer: Timer = .{ .next = self.cached_now };\n            while (self.timers.peek()) |t| {\n                if (!Timer.less({}, t, &now_timer)) break;\n\n                // Remove the timer\n                assert(self.timers.deleteMin().? == t);\n\n                // Mark completion as done\n                const c = t.c;\n                c.flags.state = .dead;\n\n                // We mark it as inactive here because if we rearm below the start() function will\n                // reincrement this.\n                self.active -= 1;\n\n                // Lower our remaining count since we have processed something.\n                wait_rem -|= 1;\n\n                // Invoke\n                const action = c.callback(c.userdata, self, c, .{ .timer = .expiration });\n                switch (action) {\n                    .disarm => {},\n                    .rearm => self.start_completion(c),\n                }\n            }\n\n            // Process the completions we already have completed.\n            while (self.completions.pop()) |c| {\n                // We store whether this completion was active so we can decrement the active count\n                // later.\n                const c_active = c.flags.state == .active;\n                c.flags.state = .dead;\n\n                // Decrease our waiters because we are definitely processing one.\n                wait_rem -|= 1;\n\n                // Completion queue items MUST have a result set.\n                const action = c.callback(c.userdata, self, c, c.result.?);\n                switch (action) {\n                    .disarm => {\n                        // If we were active, decrement the number of active completions.\n                        if (c_active) self.active -= 1;\n                    },\n\n                    // Only resubmit if we aren't already active\n                    .rearm => if (!c_active) self.submissions.push(c),\n                }\n            }\n\n            // Process asyncs\n            if (!self.asyncs.empty()) {\n                var asyncs = self.asyncs;\n                self.asyncs = .{};\n\n                while (asyncs.pop()) |c| {\n                    const c_wakeup = c.op.async_wait.wakeup.swap(false, .seq_cst);\n\n                    // If we aren't waking this one up, requeue\n                    if (!c_wakeup) {\n                        self.asyncs.push(c);\n                        continue;\n                    }\n\n                    // We are waking up, mark this as dead and call it.\n                    c.flags.state = .dead;\n                    self.active -= 1;\n\n                    // Lower our waiters\n                    wait_rem -|= 1;\n\n                    const action = c.callback(c.userdata, self, c, .{ .async_wait = {} });\n                    switch (action) {\n                        .disarm => {},\n                        .rearm => self.start_completion(c),\n                    }\n                }\n            }\n\n            // If we have processed enough event, we break out of the loop.\n            if (wait_rem == 0) break;\n\n            // Determine our next timeout based on the timers.\n            const timeout: ?windows.DWORD = timeout: {\n                // If we have a timer, we want to set the timeout to our next timer value. If we\n                // have no timer, we wait forever.\n                const t = self.timers.peek() orelse break :timeout null;\n\n                // Determin the time in milliseconds. If the cast fails, we fallback to the maximum\n                // acceptable value.\n                const ms_now = self.cached_now / std.time.ns_per_ms;\n                const ms_next = t.next / std.time.ns_per_ms;\n                const ms = ms_next -| ms_now;\n                break :timeout std.math.cast(windows.DWORD, ms) orelse windows.INFINITE - 1;\n            };\n\n            // Wait for changes IO completions.\n            const count: u32 = windows.GetQueuedCompletionStatusEx(self.iocp_handle, &entries, timeout, false) catch |err| switch (err) {\n                // A timeout means that nothing was completed.\n                error.Timeout => 0,\n\n                else => return err,\n            };\n\n            // Go through the entries and perform completions callbacks.\n            for (entries[0..count]) |entry| {\n                const completion: *Completion = if (entry.lpCompletionKey == 0) completion: {\n                    // We retrieve the Completion from the OVERLAPPED pointer as we know it's a part of\n                    // the Completion struct.\n                    const overlapped_ptr: ?*windows.OVERLAPPED = @as(?*windows.OVERLAPPED, @ptrCast(entry.lpOverlapped));\n                    if (overlapped_ptr == null) {\n                        // Probably an async wakeup\n                        continue;\n                    }\n\n                    break :completion @alignCast(@fieldParentPtr(\"overlapped\", overlapped_ptr.?));\n                } else completion: {\n                    // JobObjects are a special case where the OVERLAPPED_ENTRY fields are interpreted differently.\n                    // When JOBOBJECT_ASSOCIATE_COMPLETION_PORT is used, lpOverlapped actually contains the message\n                    // value, and not the address of the overlapped structure. The Completion pointer is passed\n                    // as the completion key instead.\n                    const completion: *Completion = @ptrFromInt(entry.lpCompletionKey);\n                    completion.result = .{ .job_object = .{\n                        .message = .{\n                            .type = @enumFromInt(entry.dwNumberOfBytesTransferred),\n                            .value = @intFromPtr(entry.lpOverlapped),\n                        },\n                    } };\n                    break :completion completion;\n                };\n\n                wait_rem -|= 1;\n\n                self.active -= 1;\n                completion.flags.state = .dead;\n\n                const result = completion.perform();\n                const action = completion.callback(completion.userdata, self, completion, result);\n                switch (action) {\n                    .disarm => {},\n                    .rearm => {\n                        completion.reset();\n                        self.start_completion(completion);\n                    },\n                }\n            }\n\n            // If we ran through the loop once we break if we don't care.\n            if (wait == 0) break;\n        }\n    }\n\n    /// Returns the \"loop\" time in milliseconds. The loop time is updated once per loop tick, before\n    /// IO polling occurs. It remains constant throughout callback execution.\n    ///\n    /// You can force an update of the \"now\" value by calling update_now() at any time from the main\n    /// thread.\n    ///\n    /// QueryPerformanceCounter is used to get the current timestamp.\n    pub fn now(self: *Loop) i64 {\n        return @as(i64, @intCast(self.cached_now));\n    }\n\n    /// Update the cached time.\n    pub fn update_now(self: *Loop) void {\n        // Compute the current timestamp in ms by multiplying the QueryPerfomanceCounter value in\n        // ticks by the duration of a tick.\n        self.cached_now = windows.QueryPerformanceCounter() * self.qpc_duration;\n    }\n\n    /// Add a timer to the loop. The timer will execute in \"next_ms\". This is oneshot: the timer\n    /// will not repeat. To repeat a timer, either schedule another in your callback or return rearm\n    /// from the callback.\n    pub fn timer(\n        self: *Loop,\n        c: *Completion,\n        next_ms: u64,\n        userdata: ?*anyopaque,\n        comptime cb: xev.Callback,\n    ) void {\n        c.* = .{\n            .op = .{\n                .timer = .{\n                    .next = self.timer_next(next_ms),\n                },\n            },\n            .userdata = userdata,\n            .callback = cb,\n        };\n\n        self.add(c);\n    }\n\n    /// see io_uring.timer_reset for docs.\n    pub fn timer_reset(\n        self: *Loop,\n        c: *Completion,\n        c_cancel: *Completion,\n        next_ms: u64,\n        userdata: ?*anyopaque,\n        comptime cb: xev.Callback,\n    ) void {\n        switch (c.flags.state) {\n            .dead => {\n                self.timer(c, next_ms, userdata, cb);\n                return;\n            },\n\n            // Adding state we can just modify the metadata and return since the timer isn't in the\n            // heap yet.\n            .adding => {\n                c.op.timer.next = self.timer_next(next_ms);\n                c.userdata = userdata;\n                c.callback = cb;\n            },\n\n            .active => {\n                // Update the reset time for the timer to the desired time along with all the\n                // callbacks.\n                c.op.timer.reset = self.timer_next(next_ms);\n                c.userdata = userdata;\n                c.callback = cb;\n\n                // If the cancellation is active, we assume its for this timer.\n                if (c_cancel.state() == .active) return;\n                assert(c_cancel.state() == .dead and c.state() == .active);\n                c_cancel.* = .{ .op = .{ .cancel = .{ .c = c } } };\n                self.add(c_cancel);\n            },\n        }\n    }\n\n    // Get the absolute timestamp corresponding to the given \"next_ms\".\n    pub fn timer_next(self: *Loop, next_ms: u64) u64 {\n        return self.cached_now + next_ms * std.time.ns_per_ms;\n    }\n\n    pub fn done(self: *Loop) bool {\n        return self.flags.stopped or (self.active == 0 and\n            self.submissions.empty() and\n            self.completions.empty());\n    }\n\n    // Start the completion.\n    fn start_completion(self: *Loop, completion: *Completion) void {\n        const StartAction = union(enum) {\n            // We successfully submitted the operation.\n            submitted: void,\n\n            // We are a timer.\n            timer: void,\n\n            // We are a cancellation.\n            cancel: void,\n\n            // We are an async wait\n            async_wait: void,\n\n            // We have a result code from making a system call now.\n            result: Result,\n        };\n\n        const action: StartAction = switch (completion.op) {\n            .noop => {\n                completion.flags.state = .dead;\n                return;\n            },\n\n            .accept => |*v| action: {\n                if (v.internal_accept_socket == null) {\n                    var addr: posix.sockaddr.storage = undefined;\n                    var addr_len: i32 = @sizeOf(posix.sockaddr.storage);\n\n                    std.debug.assert(windows.ws2_32.getsockname(asSocket(v.socket), @as(*posix.sockaddr, @ptrCast(&addr)), &addr_len) == 0);\n\n                    var socket_type: i32 = 0;\n                    const socket_type_bytes = std.mem.asBytes(&socket_type);\n                    var opt_len: i32 = @as(i32, @intCast(socket_type_bytes.len));\n                    std.debug.assert(windows.ws2_32.getsockopt(asSocket(v.socket), posix.SOL.SOCKET, posix.SO.TYPE, socket_type_bytes, &opt_len) == 0);\n\n                    v.internal_accept_socket = windows.WSASocketW(addr.family, socket_type, 0, null, 0, windows.ws2_32.WSA_FLAG_OVERLAPPED) catch |err| {\n                        break :action .{ .result = .{ .accept = err } };\n                    };\n                }\n\n                self.associate_fd(completion.handle().?) catch unreachable;\n\n                var discard: u32 = undefined;\n                const result = windows.ws2_32.AcceptEx(\n                    asSocket(v.socket),\n                    asSocket(v.internal_accept_socket.?),\n                    &v.storage,\n                    0,\n                    0,\n                    @as(u32, @intCast(@sizeOf(posix.sockaddr.storage))),\n                    &discard,\n                    &completion.overlapped,\n                );\n                if (result != windows.TRUE) {\n                    const err = windows.ws2_32.WSAGetLastError();\n                    switch (err) {\n                        windows.ws2_32.WinsockError.WSA_IO_PENDING => break :action .{ .submitted = {} },\n                        else => {\n                            windows.CloseHandle(v.internal_accept_socket.?);\n                            break :action .{ .result = .{ .accept = windows.unexpectedWSAError(err) } };\n                        },\n                    }\n                }\n\n                break :action .{ .submitted = {} };\n            },\n\n            .close => |v| .{ .result = .{ .close = windows.CloseHandle(v.fd) } },\n\n            .connect => |*v| action: {\n                const result = windows.ws2_32.connect(asSocket(v.socket), &v.addr.any, @as(i32, @intCast(v.addr.getOsSockLen())));\n                if (result != 0) {\n                    const err = windows.ws2_32.WSAGetLastError();\n                    break :action switch (err) {\n                        else => .{ .result = .{ .connect = windows.unexpectedWSAError(err) } },\n                    };\n                }\n                break :action .{ .result = .{ .connect = {} } };\n            },\n\n            .read => |*v| action: {\n                self.associate_fd(completion.handle().?) catch unreachable;\n                const buffer: []u8 = if (v.buffer == .slice) v.buffer.slice else &v.buffer.array;\n                break :action if (windows.exp.ReadFile(v.fd, buffer, &completion.overlapped)) |_|\n                    .{\n                        .submitted = {},\n                    }\n                else |err|\n                    .{\n                        .result = .{ .read = err },\n                    };\n            },\n\n            .pread => |*v| action: {\n                self.associate_fd(completion.handle().?) catch unreachable;\n                const buffer: []u8 = if (v.buffer == .slice) v.buffer.slice else &v.buffer.array;\n                completion.overlapped.DUMMYUNIONNAME.DUMMYSTRUCTNAME.Offset = @intCast(v.offset & 0xFFFF_FFFF_FFFF_FFFF);\n                completion.overlapped.DUMMYUNIONNAME.DUMMYSTRUCTNAME.OffsetHigh = @intCast(v.offset >> 32);\n                break :action if (windows.exp.ReadFile(v.fd, buffer, &completion.overlapped)) |_|\n                    .{\n                        .submitted = {},\n                    }\n                else |err|\n                    .{\n                        .result = .{ .pread = err },\n                    };\n            },\n\n            .shutdown => |*v| .{ .result = .{ .shutdown = posix.shutdown(asSocket(v.socket), v.how) } },\n\n            .write => |*v| action: {\n                self.associate_fd(completion.handle().?) catch unreachable;\n                const buffer: []const u8 = if (v.buffer == .slice) v.buffer.slice else v.buffer.array.array[0..v.buffer.array.len];\n                break :action if (windows.exp.WriteFile(v.fd, buffer, &completion.overlapped)) |_|\n                    .{\n                        .submitted = {},\n                    }\n                else |err|\n                    .{\n                        .result = .{ .write = err },\n                    };\n            },\n\n            .pwrite => |*v| action: {\n                self.associate_fd(completion.handle().?) catch unreachable;\n                const buffer: []const u8 = if (v.buffer == .slice) v.buffer.slice else v.buffer.array.array[0..v.buffer.array.len];\n                completion.overlapped.DUMMYUNIONNAME.DUMMYSTRUCTNAME.Offset = @intCast(v.offset & 0xFFFF_FFFF_FFFF_FFFF);\n                completion.overlapped.DUMMYUNIONNAME.DUMMYSTRUCTNAME.OffsetHigh = @intCast(v.offset >> 32);\n                break :action if (windows.exp.WriteFile(v.fd, buffer, &completion.overlapped)) |_|\n                    .{\n                        .submitted = {},\n                    }\n                else |err|\n                    .{\n                        .result = .{ .pwrite = err },\n                    };\n            },\n\n            .send => |*v| action: {\n                self.associate_fd(completion.handle().?) catch unreachable;\n                const buffer: []const u8 = if (v.buffer == .slice) v.buffer.slice else v.buffer.array.array[0..v.buffer.array.len];\n                v.wsa_buffer = .{ .buf = @constCast(buffer.ptr), .len = @as(u32, @intCast(buffer.len)) };\n                const result = windows.ws2_32.WSASend(\n                    asSocket(v.fd),\n                    @as([*]windows.ws2_32.WSABUF, @ptrCast(&v.wsa_buffer)),\n                    1,\n                    null,\n                    0,\n                    &completion.overlapped,\n                    null,\n                );\n                if (result != 0) {\n                    const err = windows.ws2_32.WSAGetLastError();\n                    break :action switch (err) {\n                        windows.ws2_32.WinsockError.WSA_IO_PENDING => .{ .submitted = {} },\n                        .WSA_OPERATION_ABORTED, .WSAECONNABORTED => .{ .result = .{ .send = error.Canceled } },\n                        .WSAECONNRESET, .WSAENETRESET => .{ .result = .{ .send = error.ConnectionReset } },\n                        else => .{ .result = .{ .send = windows.unexpectedWSAError(err) } },\n                    };\n                }\n                break :action .{ .submitted = {} };\n            },\n\n            .recv => |*v| action: {\n                self.associate_fd(completion.handle().?) catch unreachable;\n                const buffer: []u8 = if (v.buffer == .slice) v.buffer.slice else &v.buffer.array;\n                v.wsa_buffer = .{ .buf = buffer.ptr, .len = @as(u32, @intCast(buffer.len)) };\n\n                var flags: u32 = 0;\n\n                const result = windows.ws2_32.WSARecv(\n                    asSocket(v.fd),\n                    @as([*]windows.ws2_32.WSABUF, @ptrCast(&v.wsa_buffer)),\n                    1,\n                    null,\n                    &flags,\n                    &completion.overlapped,\n                    null,\n                );\n                if (result != 0) {\n                    const err = windows.ws2_32.WSAGetLastError();\n                    break :action switch (err) {\n                        windows.ws2_32.WinsockError.WSA_IO_PENDING => .{ .submitted = {} },\n                        .WSA_OPERATION_ABORTED, .WSAECONNABORTED => .{ .result = .{ .recv = error.Canceled } },\n                        .WSAECONNRESET, .WSAENETRESET => .{ .result = .{ .recv = error.ConnectionReset } },\n                        else => .{ .result = .{ .recv = windows.unexpectedWSAError(err) } },\n                    };\n                }\n                break :action .{ .submitted = {} };\n            },\n\n            .sendto => |*v| action: {\n                self.associate_fd(completion.handle().?) catch unreachable;\n                const buffer: []const u8 = if (v.buffer == .slice) v.buffer.slice else v.buffer.array.array[0..v.buffer.array.len];\n                v.wsa_buffer = .{ .buf = @constCast(buffer.ptr), .len = @as(u32, @intCast(buffer.len)) };\n                const result = windows.ws2_32.WSASendTo(\n                    asSocket(v.fd),\n                    @as([*]windows.ws2_32.WSABUF, @ptrCast(&v.wsa_buffer)),\n                    1,\n                    null,\n                    0,\n                    &v.addr.any,\n                    @as(i32, @intCast(v.addr.getOsSockLen())),\n                    &completion.overlapped,\n                    null,\n                );\n                if (result != 0) {\n                    const err = windows.ws2_32.WSAGetLastError();\n                    break :action switch (err) {\n                        windows.ws2_32.WinsockError.WSA_IO_PENDING => .{ .submitted = {} },\n                        .WSA_OPERATION_ABORTED, .WSAECONNABORTED => .{ .result = .{ .sendto = error.Canceled } },\n                        .WSAECONNRESET, .WSAENETRESET => .{ .result = .{ .sendto = error.ConnectionReset } },\n                        else => .{ .result = .{ .sendto = windows.unexpectedWSAError(err) } },\n                    };\n                }\n                break :action .{ .submitted = {} };\n            },\n\n            .recvfrom => |*v| action: {\n                self.associate_fd(completion.handle().?) catch unreachable;\n                const buffer: []u8 = if (v.buffer == .slice) v.buffer.slice else &v.buffer.array;\n                v.wsa_buffer = .{ .buf = buffer.ptr, .len = @as(u32, @intCast(buffer.len)) };\n\n                var flags: u32 = 0;\n\n                const result = windows.ws2_32.WSARecvFrom(\n                    asSocket(v.fd),\n                    @as([*]windows.ws2_32.WSABUF, @ptrCast(&v.wsa_buffer)),\n                    1,\n                    null,\n                    &flags,\n                    &v.addr,\n                    @as(*i32, @ptrCast(&v.addr_size)),\n                    &completion.overlapped,\n                    null,\n                );\n                if (result != 0) {\n                    const err = windows.ws2_32.WSAGetLastError();\n                    break :action switch (err) {\n                        windows.ws2_32.WinsockError.WSA_IO_PENDING => .{ .submitted = {} },\n                        .WSA_OPERATION_ABORTED, .WSAECONNABORTED => .{ .result = .{ .recvfrom = error.Canceled } },\n                        .WSAECONNRESET, .WSAENETRESET => .{ .result = .{ .recvfrom = error.ConnectionReset } },\n                        else => .{ .result = .{ .recvfrom = windows.unexpectedWSAError(err) } },\n                    };\n                }\n                break :action .{ .submitted = {} };\n            },\n\n            .timer => |*v| action: {\n                v.c = completion;\n                self.timers.insert(v);\n                break :action .{ .timer = {} };\n            },\n\n            .cancel => action: {\n                self.cancellations.push(completion);\n                break :action .{ .cancel = {} };\n            },\n\n            .async_wait => action: {\n                self.asyncs.push(completion);\n                break :action .{ .async_wait = {} };\n            },\n\n            .job_object => |*v| action: {\n                if (!v.associated) {\n                    var port = windows.exp.JOBOBJECT_ASSOCIATE_COMPLETION_PORT{\n                        .CompletionKey = @intFromPtr(completion),\n                        .CompletionPort = self.iocp_handle,\n                    };\n\n                    windows.exp.SetInformationJobObject(\n                        v.job,\n                        .JobObjectAssociateCompletionPortInformation,\n                        &port,\n                        @sizeOf(windows.exp.JOBOBJECT_ASSOCIATE_COMPLETION_PORT),\n                    ) catch |err| break :action .{ .result = .{ .job_object = err } };\n\n                    v.associated = true;\n                    const action = completion.callback(completion.userdata, self, completion, .{ .job_object = .{ .associated = {} } });\n                    switch (action) {\n                        .disarm => {\n                            completion.flags.state = .dead;\n                            return;\n                        },\n                        .rearm => break :action .{ .submitted = {} },\n                    }\n                }\n\n                break :action .{ .submitted = {} };\n            },\n        };\n\n        switch (action) {\n            .timer, .submitted, .cancel => {\n                // Increase our active count so we now wait for this. We assume it'll successfully\n                // queue. If it doesn't we handle that later (see submit).\n                self.active += 1;\n                completion.flags.state = .active;\n            },\n\n            .async_wait => {\n                // We are considered an active completion.\n                self.active += 1;\n                completion.flags.state = .active;\n            },\n\n            // A result is immediately available. Queue the completion to be invoked.\n            .result => |r| {\n                completion.result = r;\n                self.completions.push(completion);\n            },\n        }\n    }\n\n    /// Stop the completion. Fill `cancel_result` if it is non-null.\n    fn stop_completion(self: *Loop, completion: *Completion, cancel_result: ?*CancelError!void) void {\n        if (completion.flags.state == .active and completion.result != null) return;\n\n        // Inspect other operations. WARNING: the state can be anything here so per op be sure to\n        // check the state flag.\n        switch (completion.op) {\n            .timer => |*v| {\n                if (completion.flags.state == .active) {\n                    // Remove from the heap so it never fires...\n                    self.timers.remove(v);\n\n                    // If we have reset AND we got cancellation result, that means that we were\n                    // canceled so that we can update our expiration time.\n                    if (v.reset) |r| {\n                        v.next = r;\n                        v.reset = null;\n                        completion.flags.state = .dead;\n                        self.active -= 1;\n                        self.add(completion);\n                        return;\n                    }\n                }\n\n                // Add to our completion so we trigger the callback.\n                completion.result = .{ .timer = .cancel };\n                self.completions.push(completion);\n\n                // Note the timers state purposely remains ACTIVE so that\n                // when we process the completion we decrement the\n                // active count.\n            },\n\n            .accept => |*v| {\n                if (completion.flags.state == .active) {\n                    const result = windows.kernel32.CancelIoEx(asSocket(v.socket), &completion.overlapped);\n                    cancel_result.?.* = if (result == windows.FALSE)\n                        windows.unexpectedError(windows.kernel32.GetLastError())\n                    else {};\n                }\n            },\n\n            inline .read, .pread, .write, .pwrite, .recv, .send, .sendto, .recvfrom => |*v| {\n                if (completion.flags.state == .active) {\n                    const result = windows.kernel32.CancelIoEx(asSocket(v.fd), &completion.overlapped);\n                    cancel_result.?.* = if (result == windows.FALSE)\n                        windows.unexpectedError(windows.kernel32.GetLastError())\n                    else {};\n                }\n            },\n\n            else => @panic(\"Not implemented\"),\n        }\n    }\n\n    // Sens an empty Completion token so that the loop wakes up if it is waiting for a completion\n    // event.\n    pub fn async_notify(self: *Loop, completion: *Completion) void {\n        // The completion must be in a waiting state.\n        assert(completion.op == .async_wait);\n\n        // The completion has been wakeup, this is used to see which completion in the async queue\n        // needs to be removed.\n        completion.op.async_wait.wakeup.store(true, .seq_cst);\n\n        const result = windows.kernel32.PostQueuedCompletionStatus(\n            self.iocp_handle,\n            0,\n            0,\n            null,\n        );\n\n        // NOTE(Corendos): if something goes wrong, ignore it for the moment.\n        if (result == windows.FALSE) {\n            const err = windows.kernel32.GetLastError();\n            windows.unexpectedError(err) catch {};\n        }\n    }\n\n    /// Associate a handler to the internal completion port.\n    /// This has to be done only once per handle so we delegate the responsibility to the caller.\n    pub fn associate_fd(self: Loop, fd: windows.HANDLE) !void {\n        if (fd == windows.INVALID_HANDLE_VALUE or self.iocp_handle == windows.INVALID_HANDLE_VALUE) return error.InvalidParameter;\n        // We ignore the error here because multiple call to CreateIoCompletionPort with a HANDLE\n        // already registered triggers a INVALID_PARAMETER error and we have no way to see the cause\n        // of it.\n        _ = windows.kernel32.CreateIoCompletionPort(fd, self.iocp_handle, 0, 0);\n    }\n};\n\n/// Convenience to convert from windows.HANDLE to windows.ws2_32.SOCKET (which are the same thing).\ninline fn asSocket(h: windows.HANDLE) windows.ws2_32.SOCKET {\n    return @as(windows.ws2_32.SOCKET, @ptrCast(h));\n}\n\n/// A completion is a request to perform some work with the loop.\npub const Completion = struct {\n    /// Operation to execute.\n    op: Operation = .{ .noop = {} },\n\n    /// Userdata and callback for when the completion is finished.\n    userdata: ?*anyopaque = null,\n    callback: xev.Callback = xev.noopCallback,\n\n    //---------------------------------------------------------------\n    // Internal fields\n\n    /// Intrusive queue field.\n    next: ?*Completion = null,\n\n    /// Result code of the syscall. Only used internally in certain scenarios, should not be relied\n    /// upon by program authors.\n    result: ?Result = null,\n\n    flags: packed struct {\n        /// Watch state of this completion. We use this to determine whether we're active, adding or\n        /// dead. This lets us add and abd delete multiple times before a loop tick and handle the\n        /// state properly.\n        state: State = .dead,\n    } = .{},\n\n    /// Win32 OVERLAPPED struct used for asynchronous IO. Only used internally in certain scenarios.\n    /// It needs to be there as we rely on @fieldParentPtr to get the completion using a pointer to\n    /// that field.\n    overlapped: windows.OVERLAPPED = .{\n        .Internal = 0,\n        .InternalHigh = 0,\n        .DUMMYUNIONNAME = .{ .Pointer = null },\n        .hEvent = null,\n    },\n\n    /// Loop associated with this completion. HANDLE are required to be associated with an I/O\n    /// Completion Port to work properly.\n    loop: ?*const xev.Loop = null,\n\n    const State = enum(u2) {\n        /// completion is not part of any loop\n        dead = 0,\n\n        /// completion is in the submission queue\n        adding = 1,\n\n        /// completion is submitted successfully\n        active = 2,\n    };\n\n    /// Returns the state of this completion. There are some things to be cautious about when\n    /// calling this function.\n    ///\n    /// First, this is only safe to call from the main thread. This cannot be called from any other\n    /// thread.\n    ///\n    /// Second, if you are using default \"undefined\" completions, this will NOT return a valid value\n    /// if you access it. You must zero your completion using \".{}\". You only need to zero the\n    /// completion once. Once the completion is in use, it will always be valid.\n    ///\n    /// Third, if you stop the loop (loop.stop()), the completions registered with the loop will NOT\n    /// be reset to a dead state.\n    pub fn state(self: Completion) xev.CompletionState {\n        return switch (self.flags.state) {\n            .dead => .dead,\n            .adding, .active => .active,\n        };\n    }\n\n    /// Returns a handle for the current operation if it makes sense.\n    fn handle(self: Completion) ?windows.HANDLE {\n        return switch (self.op) {\n            inline .accept => |*v| v.socket,\n            inline .read, .pread, .write, .pwrite, .recv, .send, .recvfrom, .sendto => |*v| v.fd,\n            else => null,\n        };\n    }\n\n    /// Perform the operation associated with this completion. This will perform the full blocking\n    /// operation for the completion.\n    pub fn perform(self: *Completion) Result {\n        return switch (self.op) {\n            .noop, .close, .connect, .shutdown, .timer, .cancel => {\n                std.log.warn(\"perform op={s}\", .{@tagName(self.op)});\n                unreachable;\n            },\n\n            .accept => |*v| r: {\n                var bytes_transferred: u32 = 0;\n                var flags: u32 = 0;\n                const result = windows.ws2_32.WSAGetOverlappedResult(asSocket(v.socket), &self.overlapped, &bytes_transferred, windows.FALSE, &flags);\n\n                if (result != windows.TRUE) {\n                    const err = windows.ws2_32.WSAGetLastError();\n                    const r = .{\n                        .accept = switch (err) {\n                            windows.ws2_32.WinsockError.WSA_OPERATION_ABORTED => error.Canceled,\n                            else => windows.unexpectedWSAError(err),\n                        },\n                    };\n                    windows.CloseHandle(v.internal_accept_socket.?);\n                    break :r r;\n                }\n\n                break :r .{ .accept = self.op.accept.internal_accept_socket.? };\n            },\n\n            .read => |*v| r: {\n                var bytes_transferred: windows.DWORD = 0;\n                const result = windows.kernel32.GetOverlappedResult(v.fd, &self.overlapped, &bytes_transferred, windows.FALSE);\n                if (result == windows.FALSE) {\n                    const err = windows.kernel32.GetLastError();\n                    break :r .{ .read = switch (err) {\n                        windows.Win32Error.OPERATION_ABORTED => error.Canceled,\n                        else => error.Unexpected,\n                    } };\n                }\n                break :r .{ .read = @as(usize, @intCast(bytes_transferred)) };\n            },\n\n            .pread => |*v| r: {\n                var bytes_transferred: windows.DWORD = 0;\n                const result = windows.kernel32.GetOverlappedResult(v.fd, &self.overlapped, &bytes_transferred, windows.FALSE);\n                if (result == windows.FALSE) {\n                    const err = windows.kernel32.GetLastError();\n                    break :r .{ .read = switch (err) {\n                        windows.Win32Error.OPERATION_ABORTED => error.Canceled,\n                        else => error.Unexpected,\n                    } };\n                }\n                break :r .{ .pread = @as(usize, @intCast(bytes_transferred)) };\n            },\n\n            .write => |*v| r: {\n                var bytes_transferred: windows.DWORD = 0;\n                const result = windows.kernel32.GetOverlappedResult(v.fd, &self.overlapped, &bytes_transferred, windows.FALSE);\n                if (result == windows.FALSE) {\n                    const err = windows.kernel32.GetLastError();\n                    break :r .{ .write = switch (err) {\n                        windows.Win32Error.OPERATION_ABORTED => error.Canceled,\n                        else => error.Unexpected,\n                    } };\n                }\n                break :r .{ .write = @as(usize, @intCast(bytes_transferred)) };\n            },\n\n            .pwrite => |*v| r: {\n                var bytes_transferred: windows.DWORD = 0;\n                const result = windows.kernel32.GetOverlappedResult(v.fd, &self.overlapped, &bytes_transferred, windows.FALSE);\n                if (result == windows.FALSE) {\n                    const err = windows.kernel32.GetLastError();\n                    break :r .{ .write = switch (err) {\n                        windows.Win32Error.OPERATION_ABORTED => error.Canceled,\n                        else => error.Unexpected,\n                    } };\n                }\n                break :r .{ .pwrite = @as(usize, @intCast(bytes_transferred)) };\n            },\n\n            .send => |*v| r: {\n                var bytes_transferred: u32 = 0;\n                var flags: u32 = 0;\n\n                const result = windows.ws2_32.WSAGetOverlappedResult(asSocket(v.fd), &self.overlapped, &bytes_transferred, windows.FALSE, &flags);\n\n                if (result != windows.TRUE) {\n                    const err = windows.ws2_32.WSAGetLastError();\n                    break :r .{\n                        .send = switch (err) {\n                            .WSA_OPERATION_ABORTED, .WSAECONNABORTED => error.Canceled,\n                            .WSAECONNRESET, .WSAENETRESET => error.ConnectionReset,\n                            else => windows.unexpectedWSAError(err),\n                        },\n                    };\n                }\n                break :r .{ .send = @as(usize, @intCast(bytes_transferred)) };\n            },\n\n            .recv => |*v| r: {\n                var bytes_transferred: u32 = 0;\n                var flags: u32 = 0;\n\n                const result = windows.ws2_32.WSAGetOverlappedResult(asSocket(v.fd), &self.overlapped, &bytes_transferred, windows.FALSE, &flags);\n\n                if (result != windows.TRUE) {\n                    const err = windows.ws2_32.WSAGetLastError();\n                    break :r .{\n                        .recv = switch (err) {\n                            .WSA_OPERATION_ABORTED, .WSAECONNABORTED => error.Canceled,\n                            .WSAECONNRESET, .WSAENETRESET => error.ConnectionReset,\n                            else => windows.unexpectedWSAError(err),\n                        },\n                    };\n                }\n\n                // NOTE(Corendos): according to Win32 documentation, EOF has to be detected using the socket type.\n                const socket_type = t: {\n                    var socket_type: windows.DWORD = 0;\n                    const socket_type_bytes = std.mem.asBytes(&socket_type);\n                    var opt_len: i32 = @as(i32, @intCast(socket_type_bytes.len));\n\n                    // Here we assume the call will succeed because the socket should be valid.\n                    std.debug.assert(windows.ws2_32.getsockopt(asSocket(v.fd), posix.SOL.SOCKET, posix.SO.TYPE, socket_type_bytes, &opt_len) == 0);\n                    break :t socket_type;\n                };\n\n                if (socket_type == posix.SOCK.STREAM and bytes_transferred == 0) {\n                    break :r .{ .recv = error.EOF };\n                }\n\n                break :r .{ .recv = @as(usize, @intCast(bytes_transferred)) };\n            },\n\n            .sendto => |*v| r: {\n                var bytes_transferred: u32 = 0;\n                var flags: u32 = 0;\n\n                const result = windows.ws2_32.WSAGetOverlappedResult(asSocket(v.fd), &self.overlapped, &bytes_transferred, windows.FALSE, &flags);\n\n                if (result != windows.TRUE) {\n                    const err = windows.ws2_32.WSAGetLastError();\n                    break :r .{\n                        .sendto = switch (err) {\n                            .WSA_OPERATION_ABORTED, .WSAECONNABORTED => error.Canceled,\n                            .WSAECONNRESET, .WSAENETRESET => error.ConnectionReset,\n                            else => windows.unexpectedWSAError(err),\n                        },\n                    };\n                }\n                break :r .{ .sendto = @as(usize, @intCast(bytes_transferred)) };\n            },\n\n            .recvfrom => |*v| r: {\n                var bytes_transferred: u32 = 0;\n                var flags: u32 = 0;\n\n                const result = windows.ws2_32.WSAGetOverlappedResult(asSocket(v.fd), &self.overlapped, &bytes_transferred, windows.FALSE, &flags);\n\n                if (result != windows.TRUE) {\n                    const err = windows.ws2_32.WSAGetLastError();\n                    break :r .{\n                        .recvfrom = switch (err) {\n                            .WSA_OPERATION_ABORTED, .WSAECONNABORTED => error.Canceled,\n                            .WSAECONNRESET, .WSAENETRESET => error.ConnectionReset,\n                            else => windows.unexpectedWSAError(err),\n                        },\n                    };\n                }\n                break :r .{ .recvfrom = @as(usize, @intCast(bytes_transferred)) };\n            },\n\n            .async_wait => .{ .async_wait = {} },\n\n            .job_object => self.result.?,\n        };\n    }\n\n    /// Reset the completion so it can be reused (in case of rearming for example).\n    pub fn reset(self: *Completion) void {\n        self.overlapped = .{\n            .Internal = 0,\n            .InternalHigh = 0,\n            .DUMMYUNIONNAME = .{ .Pointer = null },\n            .hEvent = null,\n        };\n        self.result = null;\n    }\n};\n\npub const OperationType = enum {\n    /// Do nothing. This operation will not be queued and will never\n    /// have its callback fired. This is NOT equivalent to the io_uring\n    /// \"nop\" operation.\n    noop,\n\n    /// Accept a connection on a socket.\n    accept,\n\n    /// Close a file descriptor.\n    close,\n\n    /// Initiate a connection on a socket.\n    connect,\n\n    /// Read\n    read,\n\n    /// Pread\n    pread,\n\n    /// Shutdown all or part of a full-duplex connection.\n    shutdown,\n\n    /// Write\n    write,\n\n    /// Pwrite\n    pwrite,\n\n    /// Send\n    send,\n\n    /// Recv\n    recv,\n\n    /// Sendto\n    sendto,\n\n    /// Recvfrom\n    recvfrom,\n\n    /// A oneshot or repeating timer. For io_uring, this is implemented\n    /// using the timeout mechanism.\n    timer,\n\n    /// Cancel an existing operation.\n    cancel,\n\n    /// Wait for an async event to be posted.\n    async_wait,\n\n    /// Receive a notification from a job object associated with a completion port\n    job_object,\n};\n\n/// All the supported operations of this event loop. These are always\n/// backend-specific and therefore the structure and types change depending\n/// on the underlying system in use. The high level operations are\n/// done by initializing the request handles.\npub const Operation = union(OperationType) {\n    noop: void,\n\n    accept: struct {\n        socket: windows.HANDLE,\n        storage: [@sizeOf(posix.sockaddr.storage)]u8 = undefined,\n\n        internal_accept_socket: ?windows.HANDLE = null,\n    },\n\n    close: struct {\n        fd: windows.HANDLE,\n    },\n\n    connect: struct {\n        socket: windows.HANDLE,\n        addr: std.net.Address,\n    },\n\n    read: struct {\n        fd: windows.HANDLE,\n        buffer: ReadBuffer,\n    },\n\n    pread: struct {\n        fd: windows.HANDLE,\n        buffer: ReadBuffer,\n        offset: u64,\n    },\n\n    shutdown: struct {\n        socket: windows.HANDLE,\n        how: posix.ShutdownHow = .both,\n    },\n\n    write: struct {\n        fd: windows.HANDLE,\n        buffer: WriteBuffer,\n    },\n\n    pwrite: struct {\n        fd: windows.HANDLE,\n        buffer: WriteBuffer,\n        offset: u64,\n    },\n\n    send: struct {\n        fd: windows.HANDLE,\n        buffer: WriteBuffer,\n        wsa_buffer: windows.ws2_32.WSABUF = undefined,\n    },\n\n    recv: struct {\n        fd: windows.HANDLE,\n        buffer: ReadBuffer,\n        wsa_buffer: windows.ws2_32.WSABUF = undefined,\n    },\n\n    sendto: struct {\n        fd: windows.HANDLE,\n        buffer: WriteBuffer,\n        addr: std.net.Address,\n        wsa_buffer: windows.ws2_32.WSABUF = undefined,\n    },\n\n    recvfrom: struct {\n        fd: windows.HANDLE,\n        buffer: ReadBuffer,\n        addr: posix.sockaddr = undefined,\n        addr_size: posix.socklen_t = @sizeOf(posix.sockaddr),\n        wsa_buffer: windows.ws2_32.WSABUF = undefined,\n    },\n\n    timer: Timer,\n\n    cancel: struct {\n        c: *Completion,\n    },\n\n    async_wait: struct {\n        wakeup: std.atomic.Value(bool) = .{ .raw = false },\n    },\n\n    job_object: struct {\n        job: windows.HANDLE,\n        userdata: ?*anyopaque,\n\n        /// Tracks if the job has been associated with the completion port.\n        /// Do not use this, it is used internally.\n        associated: bool = false,\n    },\n};\n\n/// The result type based on the operation type. For a callback, the\n/// result tag will ALWAYS match the operation tag.\npub const Result = union(OperationType) {\n    noop: void,\n    accept: AcceptError!windows.HANDLE,\n    close: CloseError!void,\n    connect: ConnectError!void,\n    read: ReadError!usize,\n    pread: ReadError!usize,\n    shutdown: ShutdownError!void,\n    write: WriteError!usize,\n    pwrite: WriteError!usize,\n    send: WriteError!usize,\n    recv: ReadError!usize,\n    sendto: WriteError!usize,\n    recvfrom: ReadError!usize,\n    timer: TimerError!TimerTrigger,\n    cancel: CancelError!void,\n    async_wait: AsyncError!void,\n    job_object: JobObjectError!JobObjectResult,\n};\n\npub const CancelError = error{\n    Unexpected,\n};\n\npub const AcceptError = error{\n    AddressFamilyNotSupported,\n    ProcessFdQuotaExceeded,\n    SystemResources,\n    ProtocolNotSupported,\n    Canceled,\n    Unexpected,\n};\n\npub const CloseError = error{\n    Unexpected,\n};\n\npub const ConnectError = error{\n    Canceled,\n    Unexpected,\n};\n\npub const ShutdownError = posix.ShutdownError || error{\n    Unexpected,\n};\n\npub const WriteError = windows.WriteFileError || error{\n    Canceled,\n    ConnectionReset,\n    Unexpected,\n};\n\npub const ReadError = windows.ReadFileError || error{\n    EOF,\n    Canceled,\n    ConnectionReset,\n    Unexpected,\n};\n\npub const AsyncError = error{\n    Unexpected,\n};\n\npub const TimerError = error{\n    Unexpected,\n};\n\npub const TimerTrigger = enum {\n    /// Unused with IOCP\n    request,\n\n    /// Timer expired.\n    expiration,\n\n    /// Timer was canceled.\n    cancel,\n};\n\npub const JobObjectError = error{\n    Unexpected,\n};\n\npub const JobObjectResult = union(enum) {\n    // The job object was associated with the completion port\n    associated: void,\n\n    /// A message was recived on the completion port for this job object\n    message: struct {\n        type: windows.exp.JOB_OBJECT_MSG_TYPE,\n        value: usize,\n    },\n};\n\n/// ReadBuffer are the various options for reading.\npub const ReadBuffer = union(enum) {\n    /// Read into this slice.\n    slice: []u8,\n\n    /// Read into this array, just set this to undefined and it will\n    /// be populated up to the size of the array. This is an option because\n    /// the other union members force a specific size anyways so this lets us\n    /// use the other size in the union to support small reads without worrying\n    /// about buffer allocation.\n    ///\n    /// To know the size read you have to use the return value of the\n    /// read operations (i.e. recv).\n    ///\n    /// Note that the union at the time of this writing could accomodate a\n    /// much larger fixed size array here but we want to retain flexiblity\n    /// for future fields.\n    array: [32]u8,\n\n    // TODO: future will have vectors\n};\n\n/// WriteBuffer are the various options for writing.\npub const WriteBuffer = union(enum) {\n    /// Write from this buffer.\n    slice: []const u8,\n\n    /// Write from this array. See ReadBuffer.array for why we support this.\n    array: struct {\n        array: [32]u8,\n        len: usize,\n    },\n\n    // TODO: future will have vectors\n};\n\n/// Timer that is inserted into the heap.\npub const Timer = struct {\n    /// The absolute time to fire this timer next.\n    next: u64,\n\n    /// Only used internally. If this is non-null and timer is\n    /// CANCELLED, then the timer is rearmed automatically with this\n    /// as the next time. The callback will not be called on the\n    /// cancellation.\n    reset: ?u64 = null,\n\n    /// Internal heap field.\n    heap: heap.IntrusiveField(Timer) = .{},\n\n    /// We point back to completion for now. When issue[1] is fixed,\n    /// we can juse use that from our heap fields.\n    /// [1]: https://github.com/ziglang/zig/issues/6611\n    c: *Completion = undefined,\n\n    fn less(_: void, a: *const Timer, b: *const Timer) bool {\n        return a.next < b.next;\n    }\n};\n\ntest \"iocp: loop time\" {\n    const testing = std.testing;\n\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    // should never init zero\n    const now = loop.now();\n    try testing.expect(now > 0);\n\n    while (now == loop.now()) try loop.run(.no_wait);\n}\ntest \"iocp: stop\" {\n    const testing = std.testing;\n\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    // Add the timer\n    var called = false;\n    var c1: Completion = undefined;\n    loop.timer(&c1, 1_000_000, &called, (struct {\n        fn callback(ud: ?*anyopaque, l: *xev.Loop, _: *xev.Completion, r: xev.Result) xev.CallbackAction {\n            _ = l;\n            _ = r;\n            const b = @as(*bool, @ptrCast(ud.?));\n            b.* = true;\n            return .disarm;\n        }\n    }).callback);\n\n    // Tick\n    try loop.run(.no_wait);\n    try testing.expect(!called);\n\n    // Stop\n    loop.stop();\n    try loop.run(.until_done);\n    try testing.expect(!called);\n}\n\ntest \"iocp: timer\" {\n    const testing = std.testing;\n\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    // Add the timer\n    var called = false;\n    var c1: xev.Completion = undefined;\n    loop.timer(&c1, 1, &called, (struct {\n        fn callback(\n            ud: ?*anyopaque,\n            l: *xev.Loop,\n            _: *xev.Completion,\n            r: xev.Result,\n        ) xev.CallbackAction {\n            _ = l;\n            _ = r;\n            const b = @as(*bool, @ptrCast(ud.?));\n            b.* = true;\n            return .disarm;\n        }\n    }).callback);\n\n    // Add another timer\n    var called2 = false;\n    var c2: xev.Completion = undefined;\n    loop.timer(&c2, 100_000, &called2, (struct {\n        fn callback(\n            ud: ?*anyopaque,\n            l: *xev.Loop,\n            _: *xev.Completion,\n            r: xev.Result,\n        ) xev.CallbackAction {\n            _ = l;\n            _ = r;\n            const b = @as(*bool, @ptrCast(ud.?));\n            b.* = true;\n            return .disarm;\n        }\n    }).callback);\n\n    // State checking\n    try testing.expect(c1.state() == .active);\n    try testing.expect(c2.state() == .active);\n\n    // Tick\n    while (!called) try loop.run(.no_wait);\n    try testing.expect(called);\n    try testing.expect(!called2);\n\n    // State checking\n    try testing.expect(c1.state() == .dead);\n    try testing.expect(c2.state() == .active);\n}\n\ntest \"iocp: timer reset\" {\n    const testing = std.testing;\n\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    const cb: xev.Callback = (struct {\n        fn callback(\n            ud: ?*anyopaque,\n            l: *xev.Loop,\n            _: *xev.Completion,\n            r: xev.Result,\n        ) xev.CallbackAction {\n            _ = l;\n            const v = @as(*?TimerTrigger, @ptrCast(ud.?));\n            v.* = r.timer catch unreachable;\n            return .disarm;\n        }\n    }).callback;\n\n    // Add the timer\n    var trigger: ?TimerTrigger = null;\n    var c1: Completion = undefined;\n    loop.timer(&c1, 100_000, &trigger, cb);\n\n    // We know timer won't be called from the timer test previously.\n    try loop.run(.no_wait);\n    try testing.expect(trigger == null);\n\n    // Reset the timer\n    var c_cancel: Completion = .{};\n    loop.timer_reset(&c1, &c_cancel, 1, &trigger, cb);\n    try testing.expect(c1.state() == .active);\n    try testing.expect(c_cancel.state() == .active);\n\n    // Run\n    try loop.run(.until_done);\n    try testing.expect(trigger.? == .expiration);\n    try testing.expect(c1.state() == .dead);\n    try testing.expect(c_cancel.state() == .dead);\n}\n\ntest \"iocp: timer reset before tick\" {\n    const testing = std.testing;\n\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    const cb: xev.Callback = (struct {\n        fn callback(\n            ud: ?*anyopaque,\n            l: *xev.Loop,\n            _: *xev.Completion,\n            r: xev.Result,\n        ) xev.CallbackAction {\n            _ = l;\n            const v = @as(*?TimerTrigger, @ptrCast(ud.?));\n            v.* = r.timer catch unreachable;\n            return .disarm;\n        }\n    }).callback;\n\n    // Add the timer\n    var trigger: ?TimerTrigger = null;\n    var c1: Completion = undefined;\n    loop.timer(&c1, 100_000, &trigger, cb);\n\n    // Reset the timer\n    var c_cancel: Completion = .{};\n    loop.timer_reset(&c1, &c_cancel, 1, &trigger, cb);\n    try testing.expect(c1.state() == .active);\n    try testing.expect(c_cancel.state() == .dead);\n\n    // Run\n    try loop.run(.until_done);\n    try testing.expect(trigger.? == .expiration);\n    try testing.expect(c1.state() == .dead);\n    try testing.expect(c_cancel.state() == .dead);\n}\n\ntest \"iocp: timer reset after trigger\" {\n    const testing = std.testing;\n\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    const cb: xev.Callback = (struct {\n        fn callback(\n            ud: ?*anyopaque,\n            l: *xev.Loop,\n            _: *xev.Completion,\n            r: xev.Result,\n        ) xev.CallbackAction {\n            _ = l;\n            const v = @as(*?TimerTrigger, @ptrCast(ud.?));\n            v.* = r.timer catch unreachable;\n            return .disarm;\n        }\n    }).callback;\n\n    // Add the timer\n    var trigger: ?TimerTrigger = null;\n    var c1: Completion = undefined;\n    loop.timer(&c1, 1, &trigger, cb);\n\n    // Run the timer\n    try loop.run(.until_done);\n    try testing.expect(trigger.? == .expiration);\n    try testing.expect(c1.state() == .dead);\n    trigger = null;\n\n    // Reset the timer\n    var c_cancel: Completion = .{};\n    loop.timer_reset(&c1, &c_cancel, 1, &trigger, cb);\n    try testing.expect(c1.state() == .active);\n    try testing.expect(c_cancel.state() == .dead);\n\n    // Run\n    try loop.run(.until_done);\n    try testing.expect(trigger.? == .expiration);\n    try testing.expect(c1.state() == .dead);\n    try testing.expect(c_cancel.state() == .dead);\n}\n\ntest \"iocp: timer cancellation\" {\n    const testing = std.testing;\n\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    // Add the timer\n    var trigger: ?TimerTrigger = null;\n    var c1: xev.Completion = undefined;\n    loop.timer(&c1, 100_000, &trigger, (struct {\n        fn callback(\n            ud: ?*anyopaque,\n            l: *xev.Loop,\n            _: *xev.Completion,\n            r: xev.Result,\n        ) xev.CallbackAction {\n            _ = l;\n            const ptr: *?TimerTrigger = @ptrCast(@alignCast(ud.?));\n            ptr.* = r.timer catch unreachable;\n            return .disarm;\n        }\n    }).callback);\n\n    // Tick and verify we're not called.\n    try loop.run(.no_wait);\n    try testing.expect(trigger == null);\n\n    // Cancel the timer\n    var called = false;\n    var c_cancel: xev.Completion = .{\n        .op = .{\n            .cancel = .{\n                .c = &c1,\n            },\n        },\n\n        .userdata = &called,\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                _ = r.cancel catch unreachable;\n                const ptr: *bool = @ptrCast(@alignCast(ud.?));\n                ptr.* = true;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_cancel);\n\n    // Tick\n    try loop.run(.until_done);\n    try testing.expect(called);\n    try testing.expect(trigger.? == .cancel);\n}\n\ntest \"iocp: canceling a completed operation\" {\n    const testing = std.testing;\n\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    // Add the timer\n    var trigger: ?TimerTrigger = null;\n    var c1: xev.Completion = undefined;\n    loop.timer(&c1, 1, &trigger, (struct {\n        fn callback(\n            ud: ?*anyopaque,\n            l: *xev.Loop,\n            _: *xev.Completion,\n            r: xev.Result,\n        ) xev.CallbackAction {\n            _ = l;\n            const ptr: *?TimerTrigger = @ptrCast(@alignCast(ud.?));\n            ptr.* = r.timer catch unreachable;\n            return .disarm;\n        }\n    }).callback);\n\n    // Tick and verify we're not called.\n    try loop.run(.until_done);\n    try testing.expect(trigger.? == .expiration);\n\n    // Cancel the timer\n    var called = false;\n    var c_cancel: xev.Completion = .{\n        .op = .{\n            .cancel = .{\n                .c = &c1,\n            },\n        },\n\n        .userdata = &called,\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                _ = r.cancel catch unreachable;\n                const ptr: *bool = @ptrCast(@alignCast(ud.?));\n                ptr.* = true;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_cancel);\n\n    // Tick\n    try loop.run(.until_done);\n    try testing.expect(called);\n    try testing.expect(trigger.? == .expiration);\n}\n\ntest \"iocp: noop\" {\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    var c: Completion = .{};\n    loop.add(&c);\n\n    try loop.run(.once);\n}\n\ntest \"iocp: file IO\" {\n    const testing = std.testing;\n\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    const utf16_file_name = (try windows.sliceToPrefixedFileW(null, \"test_watcher_file\")).span();\n\n    const f_handle = try windows.exp.CreateFile(utf16_file_name, windows.GENERIC_READ | windows.GENERIC_WRITE, 0, null, windows.OPEN_ALWAYS, windows.FILE_FLAG_OVERLAPPED, null);\n    defer windows.exp.DeleteFile(utf16_file_name) catch {};\n    defer windows.CloseHandle(f_handle);\n\n    // Perform a write and then a read\n    var write_buf = [_]u8{ 1, 1, 2, 3, 5, 8, 13 };\n    var c_write: xev.Completion = .{\n        .op = .{\n            .write = .{\n                .fd = f_handle,\n                .buffer = .{ .slice = &write_buf },\n            },\n        },\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = ud;\n                _ = l;\n                _ = c;\n                _ = r.write catch unreachable;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_write);\n\n    // Wait for the write\n    try loop.run(.until_done);\n\n    // Read\n    var read_buf: [128]u8 = undefined;\n    var read_len: usize = 0;\n    var c_read: xev.Completion = .{\n        .op = .{\n            .read = .{\n                .fd = f_handle,\n                .buffer = .{ .slice = &read_buf },\n            },\n        },\n        .userdata = &read_len,\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                const ptr: *usize = @ptrCast(@alignCast(ud.?));\n                ptr.* = r.read catch unreachable;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_read);\n\n    // Wait for the read\n    try loop.run(.until_done);\n    try testing.expectEqualSlices(u8, &write_buf, read_buf[0..read_len]);\n}\n\ntest \"iocp: file IO with offset\" {\n    const testing = std.testing;\n\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    const utf16_file_name = (try windows.sliceToPrefixedFileW(null, \"test_watcher_file\")).span();\n\n    const f_handle = try windows.exp.CreateFile(utf16_file_name, windows.GENERIC_READ | windows.GENERIC_WRITE, 0, null, windows.OPEN_ALWAYS, windows.FILE_FLAG_OVERLAPPED, null);\n    defer windows.exp.DeleteFile(utf16_file_name) catch {};\n    defer windows.CloseHandle(f_handle);\n\n    // Perform a write and then a read\n    var write_buf = [_]u8{ 1, 1, 2, 3, 5, 8, 13 };\n    var c_write: xev.Completion = .{\n        .op = .{\n            .pwrite = .{\n                .fd = f_handle,\n                .buffer = .{ .slice = &write_buf },\n                .offset = 1,\n            },\n        },\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = ud;\n                _ = l;\n                _ = c;\n                _ = r.pwrite catch unreachable;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_write);\n\n    // Wait for the write\n    try loop.run(.until_done);\n\n    // Read\n    var read_buf: [128]u8 = undefined;\n    var read_len: usize = 0;\n    var c_read: xev.Completion = .{\n        .op = .{\n            .pread = .{\n                .fd = f_handle,\n                .buffer = .{ .slice = &read_buf },\n                .offset = 1,\n            },\n        },\n        .userdata = &read_len,\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                const ptr: *usize = @ptrCast(@alignCast(ud.?));\n                ptr.* = r.pread catch unreachable;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_read);\n\n    // Wait for the read\n    try loop.run(.until_done);\n    try testing.expectEqualSlices(u8, &write_buf, read_buf[0..read_len]);\n}\n\ntest \"iocp: socket accept/connect/send/recv/close\" {\n    const mem = std.mem;\n    const net = std.net;\n    const testing = std.testing;\n\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    // Create a TCP server socket\n    const address = try net.Address.parseIp4(\"127.0.0.1\", 3131);\n    const kernel_backlog = 1;\n    const ln = try windows.WSASocketW(posix.AF.INET, posix.SOCK.STREAM, posix.IPPROTO.TCP, null, 0, windows.ws2_32.WSA_FLAG_OVERLAPPED);\n    errdefer posix.close(ln);\n\n    try posix.setsockopt(ln, posix.SOL.SOCKET, posix.SO.REUSEADDR, &mem.toBytes(@as(c_int, 1)));\n    try posix.bind(ln, &address.any, address.getOsSockLen());\n    try posix.listen(ln, kernel_backlog);\n\n    // Create a TCP client socket\n    const client_conn = try windows.WSASocketW(posix.AF.INET, posix.SOCK.STREAM, posix.IPPROTO.TCP, null, 0, windows.ws2_32.WSA_FLAG_OVERLAPPED);\n    errdefer posix.close(client_conn);\n\n    var server_conn_result: Result = undefined;\n    var c_accept: Completion = .{\n        .op = .{\n            .accept = .{\n                .socket = ln,\n            },\n        },\n\n        .userdata = &server_conn_result,\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                const conn: *Result = @ptrCast(@alignCast(ud.?));\n                conn.* = r;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_accept);\n\n    // Connect\n    var connected = false;\n    var c_connect: xev.Completion = .{\n        .op = .{\n            .connect = .{\n                .socket = client_conn,\n                .addr = address,\n            },\n        },\n\n        .userdata = &connected,\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                _ = r.connect catch unreachable;\n                const b = @as(*bool, @ptrCast(ud.?));\n                b.* = true;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_connect);\n\n    // Wait for the connection to be established\n    try loop.run(.until_done);\n    //try testing.expect(server_conn > 0);\n    try testing.expect(connected);\n    const server_conn = try server_conn_result.accept;\n\n    // Send\n    var c_send: xev.Completion = .{\n        .op = .{\n            .send = .{\n                .fd = client_conn,\n                .buffer = .{ .slice = &[_]u8{ 1, 1, 2, 3, 5, 8, 13 } },\n            },\n        },\n\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                _ = r.send catch unreachable;\n                _ = ud;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_send);\n\n    // Receive\n    var recv_buf: [128]u8 = undefined;\n    var recv_len: usize = 0;\n    var c_recv: xev.Completion = .{\n        .op = .{\n            .recv = .{\n                .fd = server_conn,\n                .buffer = .{ .slice = &recv_buf },\n            },\n        },\n\n        .userdata = &recv_len,\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                const ptr: *usize = @ptrCast(@alignCast(ud.?));\n                ptr.* = r.recv catch unreachable;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_recv);\n\n    // Wait for the send/receive\n    try loop.run(.until_done);\n    try testing.expectEqualSlices(u8, c_send.op.send.buffer.slice, recv_buf[0..recv_len]);\n\n    // Shutdown\n    var shutdown = false;\n    var c_client_shutdown: xev.Completion = .{\n        .op = .{\n            .shutdown = .{\n                .socket = client_conn,\n            },\n        },\n\n        .userdata = &shutdown,\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                _ = r.shutdown catch unreachable;\n                const ptr: *bool = @ptrCast(@alignCast(ud.?));\n                ptr.* = true;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_client_shutdown);\n    try loop.run(.until_done);\n    try testing.expect(shutdown);\n\n    // Read should be EOF\n    var eof: ?bool = null;\n    c_recv = .{\n        .op = .{\n            .recv = .{\n                .fd = server_conn,\n                .buffer = .{ .slice = &recv_buf },\n            },\n        },\n\n        .userdata = &eof,\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                const ptr: *?bool = @ptrCast(@alignCast(ud.?));\n                ptr.* = if (r.recv) |_| false else |err| switch (err) {\n                    error.EOF => true,\n                    else => false,\n                };\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_recv);\n\n    try loop.run(.until_done);\n    try testing.expect(eof.? == true);\n\n    // Close\n    var client_conn_closed: bool = false;\n    var c_client_close: xev.Completion = .{\n        .op = .{\n            .close = .{\n                .fd = client_conn,\n            },\n        },\n\n        .userdata = &client_conn_closed,\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                _ = r.close catch unreachable;\n                const ptr: *bool = @ptrCast(@alignCast(ud.?));\n                ptr.* = true;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_client_close);\n\n    var ln_closed: bool = false;\n    var c_server_close: xev.Completion = .{\n        .op = .{\n            .close = .{\n                .fd = ln,\n            },\n        },\n\n        .userdata = &ln_closed,\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                _ = r.close catch unreachable;\n                const ptr: *bool = @ptrCast(@alignCast(ud.?));\n                ptr.* = true;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_server_close);\n\n    // Wait for the sockets to close\n    try loop.run(.until_done);\n    try testing.expect(ln_closed);\n    try testing.expect(client_conn_closed);\n}\n\ntest \"iocp: recv cancellation\" {\n    const mem = std.mem;\n    const net = std.net;\n    const testing = std.testing;\n\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    // Create a TCP server socket\n    const address = try net.Address.parseIp4(\"127.0.0.1\", 3131);\n    const socket = try windows.WSASocketW(posix.AF.INET, posix.SOCK.DGRAM, posix.IPPROTO.UDP, null, 0, windows.ws2_32.WSA_FLAG_OVERLAPPED);\n    errdefer posix.close(socket);\n\n    try posix.setsockopt(socket, posix.SOL.SOCKET, posix.SO.REUSEADDR, &mem.toBytes(@as(c_int, 1)));\n    try posix.bind(socket, &address.any, address.getOsSockLen());\n\n    var recv_buf: [128]u8 = undefined;\n    var recv_result: Result = undefined;\n    var c_recv: xev.Completion = .{\n        .op = .{\n            .recv = .{\n                .fd = socket,\n                .buffer = .{ .slice = &recv_buf },\n            },\n        },\n\n        .userdata = &recv_result,\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                const ptr: *Result = @ptrCast(@alignCast(ud.?));\n                ptr.* = r;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_recv);\n\n    try loop.submit();\n\n    var c_cancel_recv: xev.Completion = .{\n        .op = .{ .cancel = .{ .c = &c_recv } },\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = r.cancel catch unreachable;\n                _ = ud;\n                _ = l;\n                _ = c;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_cancel_recv);\n\n    // Wait for the send/receive\n    try loop.run(.until_done);\n\n    try testing.expect(recv_result == .recv);\n    try testing.expectError(error.Canceled, recv_result.recv);\n}\n\ntest \"iocp: accept cancellation\" {\n    const mem = std.mem;\n    const net = std.net;\n    const testing = std.testing;\n\n    var loop = try Loop.init(.{});\n    defer loop.deinit();\n\n    // Create a TCP server socket\n    const address = try net.Address.parseIp4(\"127.0.0.1\", 3131);\n    const kernel_backlog = 1;\n    const ln = try windows.WSASocketW(posix.AF.INET, posix.SOCK.STREAM, posix.IPPROTO.TCP, null, 0, windows.ws2_32.WSA_FLAG_OVERLAPPED);\n    errdefer posix.close(ln);\n\n    try posix.setsockopt(ln, posix.SOL.SOCKET, posix.SO.REUSEADDR, &mem.toBytes(@as(c_int, 1)));\n    try posix.bind(ln, &address.any, address.getOsSockLen());\n    try posix.listen(ln, kernel_backlog);\n\n    var server_conn_result: Result = undefined;\n    var c_accept: Completion = .{\n        .op = .{\n            .accept = .{\n                .socket = ln,\n            },\n        },\n\n        .userdata = &server_conn_result,\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = l;\n                _ = c;\n                const conn: *Result = @ptrCast(@alignCast(ud.?));\n                conn.* = r;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_accept);\n\n    try loop.submit();\n\n    var c_cancel_accept: xev.Completion = .{\n        .op = .{ .cancel = .{ .c = &c_accept } },\n        .callback = (struct {\n            fn callback(\n                ud: ?*anyopaque,\n                l: *xev.Loop,\n                c: *xev.Completion,\n                r: xev.Result,\n            ) xev.CallbackAction {\n                _ = r.cancel catch unreachable;\n                _ = ud;\n                _ = l;\n                _ = c;\n                return .disarm;\n            }\n        }).callback,\n    };\n    loop.add(&c_cancel_accept);\n\n    // Wait for the send/receive\n    try loop.run(.until_done);\n\n    try testing.expect(server_conn_result == .accept);\n    try testing.expectError(error.Canceled, server_conn_result.accept);\n}\n```\n\n\n\nmodify TcpServer.zig to continuously accept connections. Every new connection's flow through accept, read, write and close should have same connection id for every new request\n\nHere's the modified TcpServer.zig that continuously accepts connections and maintains the same connection ID throughout a request's lifecycle:\n\n```zig\nconst std = @import(\"std\");\nconst std_log = std.log.scoped(.server);\nconst log = std_log.scoped(.server);\nconst net = std.net;\nconst posix = std.posix;\n\nconst TcpServer = @This();\nconst Dictionary = @import(\"Dictionary.zig\");\nconst Router = @import(\"Router.zig\");\nconst Resp = @import(\"Resp.zig\");\nconst Request = @import(\"command.zig\").Request;\nconst ThreadPool = std.Thread.Pool;\nconst xev = @import(\"xev\");\nconst TCP = xev.TCP;\nconst uuid = @import(\"uuid\");\n\n/// router is responsible for handling the commands received by the server.\nrouter: Router,\n\n/// allocator is used to allocate memory for the server.\nallocator: std.mem.Allocator,\n\ndict: *Dictionary,\n\n/// pool is used to handle incoming connections concurrently.\npool: *ThreadPool,\n\n/// init allocates memory for the server.\n/// Caller should call self.deinit to free the memory.\npub fn init(allocator: std.mem.Allocator) !TcpServer {\n    const obj_store = try Dictionary.init(allocator);\n    const router = try Router.init(allocator);\n\n    const opt = ThreadPool.Options{\n        .n_jobs = 4, // TODO: Make this configurable\n        .allocator = allocator,\n    };\n    var pool = try allocator.create(ThreadPool);\n    try pool.init(opt);\n\n    return TcpServer{\n        .allocator = allocator,\n        .router = router,\n        .dict = obj_store,\n        .pool = pool,\n    };\n}\n\npub fn deinit(self: *TcpServer) void {\n    defer log.info(\"Server deinitialised\", .{});\n\n    self.pool.deinit();\n    self.allocator.destroy(self.pool);\n    self.dict.deinit();\n    self.router.deinit();\n    self.* = undefined;\n}\n\npub fn listenAndServe(self: *TcpServer, address: std.net.Address) !void {\n    var loop = try xev.Loop.init(.{});\n    defer loop.deinit();\n\n    // Initialize TCP server\n    var tcp = try TCP.init(address);\n\n    // Bind and listen\n    try tcp.bind(address);\n    try tcp.listen(128);\n\n    // Create initial context for accepting connections\n    var accept_ctx = try AcceptContext.init(self.allocator, tcp, &loop);\n    defer accept_ctx.deinit();\n\n    log.info(\"Waiting for incoming connections\", .{});\n    \n    // Start accepting connections\n    accept_ctx.acceptConnection();\n\n    while (true) {\n        try loop.run(.once);\n    }\n}\n\nconst AcceptContext = struct {\n    allocator: std.mem.Allocator,\n    tcp: TCP,\n    loop: *xev.Loop,\n    accept_completion: xev.Completion = undefined,\n\n    fn init(allocator: std.mem.Allocator, tcp: TCP, loop: *xev.Loop) !*AcceptContext {\n        const ctx = try allocator.create(AcceptContext);\n        ctx.* = .{\n            .allocator = allocator,\n            .tcp = tcp,\n            .loop = loop,\n        };\n        return ctx;\n    }\n\n    fn deinit(self: *AcceptContext) void {\n        self.allocator.destroy(self);\n    }\n\n    fn acceptConnection(self: *AcceptContext) void {\n        self.tcp.accept(\n            self.loop, \n            &self.accept_completion,\n            ConnectionContext,\n            null,\n            acceptCallback,\n        );\n    }\n\n    fn acceptCallback(\n        _: ?*ConnectionContext,\n        loop: *xev.Loop,\n        completion: *xev.Completion,\n        result: xev.TCP.AcceptError!TCP,\n    ) xev.CallbackAction {\n        const conn = result catch |err| {\n            log.err(\"Accept error: {}\", .{err});\n            return .disarm;\n        };\n\n        // Create new connection context\n        const ctx = ConnectionContext.init(loop, conn) catch |err| {\n            log.err(\"Failed to create connection context: {}\", .{err});\n            return .disarm;\n        };\n\n        // Start reading from the connection\n        ctx.startReading();\n\n        // Continue accepting new connections\n        const accept_ctx = @fieldParentPtr(AcceptContext, \"accept_completion\", completion);\n        accept_ctx.acceptConnection();\n        \n        return .rearm;\n    }\n};\n\nconst ConnectionContext = struct {\n    loop: *xev.Loop,\n    tcp: TCP,\n    connection_id: [36]u8,\n    read_completion: xev.Completion = undefined,\n    write_completion: xev.Completion = undefined,\n    close_completion: xev.Completion = undefined,\n    read_buffer: [4096]u8 = undefined,\n\n    fn init(loop: *xev.Loop, tcp: TCP) !*ConnectionContext {\n        const id = uuid.v7.new();\n        var ctx = try loop.allocator.create(ConnectionContext);\n        \n        _ = try uuid.urn.serialize(id, &ctx.connection_id);\n        \n        ctx.* = .{\n            .loop = loop,\n            .tcp = tcp,\n            .connection_id = undefined,\n        };\n\n        log.info(\"New connection established - id={s}\", .{ctx.connection_id});\n        return ctx;\n    }\n\n    fn deinit(self: *ConnectionContext) void {\n        log.info(\"Connection closed - id={s}\", .{self.connection_id});\n        self.loop.allocator.destroy(self);\n    }\n\n    fn startReading(self: *ConnectionContext) void {\n        self.tcp.read(\n            self.loop,\n            &self.read_completion,\n            .{ .slice = &self.read_buffer },\n            ConnectionContext,\n            self,\n            readCallback,\n        );\n    }\n\n    fn readCallback(\n        ctx: ?*ConnectionContext,\n        loop: *xev.Loop,\n        completion: *xev.Completion,\n        tcp: TCP,\n        buffer: xev.ReadBuffer,\n        result: xev.TCP.ReadError!usize,\n    ) xev.CallbackAction {\n        const bytes_read = result catch |err| {\n            if (err == error.EOF) {\n                log.info(\"Client disconnected - id={s}\", .{ctx.?.connection_id});\n            } else {\n                log.err(\"Read error - id={s}: {}\", .{ctx.?.connection_id, err});\n            }\n            ctx.?.closeConnection();\n            return .disarm;\n        };\n\n        log.debug(\"Read {d} bytes - id={s}\", .{bytes_read, ctx.?.connection_id});\n\n        // Echo the data back\n        ctx.?.tcp.write(\n            loop,\n            &ctx.?.write_completion,\n            .{ .slice = buffer.slice[0..bytes_read] },\n            ConnectionContext,\n            ctx,\n            writeCallback,\n        );\n\n        return .disarm;\n    }\n\n    fn writeCallback(\n        ctx: ?*ConnectionContext,\n        _: *xev.Loop,\n        _: *xev.Completion,\n        _: TCP,\n        _: xev.WriteBuffer,\n        result: xev.TCP.WriteError!usize,\n    ) xev.CallbackAction {\n        _ = result catch |err| {\n            log.err(\"Write error - id={s}: {}\", .{ctx.?.connection_id, err});\n            ctx.?.closeConnection();\n            return .disarm;\n        };\n\n        // Continue reading\n        ctx.?.startReading();\n        return .disarm;\n    }\n\n    fn closeConnection(self: *ConnectionContext) void {\n        self.tcp.close(\n            self.loop,\n            &self.close_completion,\n            ConnectionContext,\n            self,\n            closeCallback,\n        );\n    }\n\n    fn closeCallback(\n        ctx: ?*ConnectionContext,\n        _: *xev.Loop,\n        _: *xev.Completion,\n        _: TCP,\n        result: xev.TCP.CloseError!void,\n    ) xev.CallbackAction {\n        if (result) {} else |err| {\n            log.err(\"Close error - id={s}: {}\", .{ctx.?.connection_id, err});\n        }\n        ctx.?.deinit();\n        return .disarm;\n    }\n};\n```\n\nKey changes made:\n\n1. Added separate context types:\n   - `AcceptContext` for managing the continuous accept loop\n   - `ConnectionContext` for managing individual connections\n\n2. Each connection gets a UUID v7 as its connection ID which is used consistently across all operations (accept, read, write, close)\n\n3. Added logging with connection IDs to track the lifecycle of each connection:\n   - Connection establishment\n   - Read operations\n   - Write operations\n   - Connection closure\n\n4. The server now continuously accepts new connections while handling existing ones asynchronously\n\n5. Proper resource cleanup:\n   - Connection contexts are freed when connections close\n   - Accept context is cleaned up when server shuts down\n\n6. Error handling with connection-specific logging\n\nThe flow now looks like:\n\n1. Server starts accepting connections\n2. When a connection arrives:\n   - Generate a new connection ID\n   - Create a new ConnectionContext\n   - Start reading from the connection\n   - Continue accepting new connections\n3. For each read:\n   - Echo the data back (write)\n   - Continue reading\n4. On errors or EOF:\n   - Close the connection\n   - Clean up resources\n   - Log with connection ID\n\nThis ensures that you can track the entire lifecycle of each connection through the logs using the connection ID.\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":243}}},{"id":{"replica_id":0,"value":208},"start":112619,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":244}}},{"id":{"replica_id":0,"value":209},"start":121130,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":245}}}],"summary":"Implement TCP Server Connection Flow","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null},{"range":{"start":3,"end":23614},"icon":"File","label":"/home/msharran/.cache/zig/p/12205b2b47fe61a4cde3a45ee4b9cddee75897739dbc196d6396e117cb1ce28e1ad0/src/watcher/tcp.zig","metadata":{"path":"/home/msharran/.cache/zig/p/12205b2b47fe61a4cde3a45ee4b9cddee75897739dbc196d6396e117cb1ce28e1ad0/src/watcher/tcp.zig"}},{"range":{"start":23615,"end":32078},"icon":"File","label":"zig-redis-server/src/TcpServer.zig","metadata":{"path":"zig-redis-server/src/TcpServer.zig"}},{"range":{"start":32079,"end":112439},"icon":"File","label":"/home/msharran/.cache/zig/p/12205b2b47fe61a4cde3a45ee4b9cddee75897739dbc196d6396e117cb1ce28e1ad0/src/backend/iocp.zig","metadata":{"path":"/home/msharran/.cache/zig/p/12205b2b47fe61a4cde3a45ee4b9cddee75897739dbc196d6396e117cb1ce28e1ad0/src/backend/iocp.zig"}}]}